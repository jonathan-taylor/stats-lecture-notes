{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic regression\n",
    "\n",
    "Binary outcomes\n",
    "\n",
    "* Most models so far have had response $Y$ as continuous.\n",
    "* Many responses in practice fall into the $YES/NO$ framework.\n",
    "* Examples:\n",
    "*     1. medical: presence or absence of cancer\n",
    "      1. financial: bankrupt or solvent\n",
    "      1. industrial: passes a quality control test or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modelling probabilities\n",
    "\n",
    "* For $0-1$ responses we need to model \n",
    "$$\\pi(x_1, \\dots, x_p) = P(Y=1|X_1=x_1,\\dots, X_p=x_p)$$\n",
    "* That is, $Y$ is Bernoulli with a probability that depends on covariates $\\{X_1, \\dots, X_p\\}.$\n",
    "* **Note:**\n",
    "   $\\text{Var}(Y) = \\pi ( 1 - \\pi) = E(Y) \\cdot ( 1-  E(Y))$\n",
    "* **Or,**\n",
    "   the binary nature forces a relation between mean and variance of $Y$.\n",
    "* This makes logistic regression a **Generalized Linear Model**, use `glm` in `R`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Flu shot example\n",
    "\n",
    "* A local health clinic sent fliers to its clients to encourage everyone, but especially older persons at high risk of complications, to get a flu shot in time for protection against an expected flu epidemic.\n",
    "* In a pilot follow-up study, 50 clients were randomly selected and asked whether they actually received a flu shot. $Y={\\tt Shot}$\n",
    "* In addition, data were collected on their age $X_1={\\tt Age}$ and their health awareness $X_2={\\tt Health.Aware}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A possible model\n",
    "\n",
    "- Simplest model $\\pi(X_1,X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$\n",
    "- Problems / issues:\n",
    "     - We must have $0 \\leq E(Y) = \\pi(X_1,X_2) \\leq 1$. OLS will not force this.\n",
    "     - Inference using Gauss model will not work because of relation between mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic model\n",
    "\n",
    "* Logistic model $\\pi(X_1,X_2) = \\frac{\\exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2)}{1 + \\exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2)}$\n",
    "* This automatically fixes $0 \\leq E(Y) = \\pi(X_1,X_2) \\leq 1$.\n",
    "* **Note:**\n",
    "   $\\text{logit}(\\pi(X_1, X_2)) = \\log\\left(\\frac{\\pi(X_1, X_2)}{1 - \\pi(X_1,X_2)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "g <- function(p) {\n",
    "  return(log(p / (1 - p)))\n",
    "}\n",
    "\n",
    "g.inv <- function(x) {\n",
    "  return(exp(x) / (1 + exp(x)))\n",
    "}\n",
    "\n",
    "x = seq(g(0.01), g(0.99), length=200)\n",
    "plot(x, g.inv(x), lwd=2, type='l', col='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p = seq(0.01,0.99,length=200)\n",
    "plot(p, g(p), lwd=2, type='l', col='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary regression models\n",
    "\n",
    "* Models $E(Y)$ as some increasing function $F$ of $\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$.\n",
    "* The logistic model uses the function $$F(x)=e^x/(1+e^x).$$\n",
    "* Can be fit using Maximum Likelihood / Iteratively Reweighted Least Squares.\n",
    "* For logistic regression, coefficients have nice interpretation in terms of `odds ratios` (to be defined shortly).\n",
    "  \n",
    "* What about inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Criterion used to fit model\n",
    "\n",
    "Instead of sum of squares, logistic regression \n",
    "uses *deviance*:\n",
    "\n",
    "* Defined as $$DEV(\\mu| Y) = -2 \\log L(\\mu| Y) + 2 \\log L(Y| Y)$$\n",
    "where $\\mu$ is a location estimator for $Y$.\n",
    "\n",
    "* If $Y$ is Gaussian with independent $N(\\mu_i,\\sigma^2)$ entries then \n",
    "$$DEV(\\mu| Y) = \\frac{1}{\\sigma^2}\\sum_{i=1}^n(Y_i - \\mu_i)^2$$\n",
    "\n",
    "* If $Y$ is a binary vector, with mean vector $\\pi$ then \n",
    "$$DEV(\\pi| Y) = -2 \\sum_{i=1}^n \\left( Y_i \\log(\\pi_i) + (1-Y_i) \\log(1-\\pi_i) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deviance for logistic regression\n",
    "\n",
    "* For any binary regression model, the deviance is:\n",
    "$$\\begin{aligned}\n",
    "     DEV(\\beta| Y) &=  -2 \\sum_{i=1}^n \\left( Y_i {\\text{logit}}(\\pi_i(\\beta)) + \\log(1-\\pi_i(\\beta)) \\right)\n",
    "     \\end{aligned}$$\n",
    "     \n",
    "* For the logistic model, the RHS is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    -2 \\left[ (X\\beta)^Ty + \\sum_{i=1}^n\\log \\left(1 + \\exp \\left(\\sum_{j=1}^p X_ij \\beta_j\\right) \\right)\\right]\n",
    "     \\end{aligned}$$\n",
    "   \n",
    "* The **logistic model is special** in that $\\text{logit}(\\pi(\\beta))=X\\beta$. If we used\n",
    "a different transformation, the first part would not be linear in $X\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "flu.table = read.table('http://stats203.stanford.edu/data/flu.table', header=T)\n",
    "flu.glm = glm(Shot ~ Age + Health.Aware, data=flu.table, family=binomial())\n",
    "print(summary(flu.glm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Odds Ratios\n",
    "\n",
    "* One reason logistic models are popular is that the parameters have simple interpretations in terms of **odds**\n",
    "   $$ODDS(A) = \\frac{P(A)}{1-P(A)}.$$\n",
    "* Logistic model: $$OR_{X_j} = \\frac{ODDS(Y=1|\\dots, X_j=x_j+1, \\dots)}{ODDS(Y=1|\\dots, X_j=x_j, \\dots)} = e^{\\beta_j}$$\n",
    "* If $X_j \\in {0, 1}$ is dichotomous, then odds for group with $X_j = 1$ are $e^{\\beta_j}$ higher, other parameters being equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Rare disease hypothesis\n",
    "\n",
    "* When incidence is rare, $P(Y=0)\\approxeq 1$ no matter what the covariates $X_j$’s are.\n",
    "* In this case, odds ratios are almost ratios of probabilities: $$OR_{X_j} \\approxeq \\frac{{\\mathbb{P}}(Y=1|\\dots, X_j=x_j+1, \\dots)}{{\\mathbb{P}}(Y=1|\\dots, X_j=x_j, \\dots)}$$\n",
    "* Hypothetical example: in a lung cancer study, if $X_j$ is an indicator of smoking or not, a $\\beta_j$ of 5 means for smoking vs. non-smoking means smokers are $e^5 \\approx 150$ times more likely to develop lung cancer\n",
    "* In flu example, the odds for a 45 year old with health awareness 50 compared to a 35 year old with the same health awareness are\n",
    "$$e^{-1.429284+3.647052}=9.18$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logodds = predict(flu.glm, list(Age=c(35,45),Health.Aware=c(50,50)))\n",
    "logodds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The estimated probabilities are below, yielding a ratio of $0.1932/0.0254 \\approx 7.61$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "exp(logodds)/(1+exp(logodds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An algorithm to fit the model\n",
    "\n",
    "### Iteratively Reweighted Least Squares (IRLS)\n",
    "\n",
    "1. Initialize $\\widehat{\\pi}_i = \\bar{Y}, 1 \\leq i \\leq n$\n",
    "2. Define $$Z_i = g(\\widehat{\\pi}_i) + g'(\\widehat{\\pi}_i) (Y_i - \\widehat{\\pi_i})$$\n",
    "3. Fit weighted least squares model \n",
    "$$Z_i = \\sum_{j=1}^p \\beta_j X_{ij}, \\qquad w_i = \\widehat{\\pi_i} (1 - \\widehat{\\pi}_i)$$\n",
    "4. Set $\\widehat{\\pi}_i = \\text{logit}^{-1} \\left(\\widehat{\\beta}_0 + \\sum_{j=1}^p \\widehat{\\beta}_j X_{ij}\\right)$.\n",
    "5. Repeat steps 2-4 until convergence. \n",
    "\n",
    "- This is essentially Newton-Raphson to minimize deviance. For logistic regression, this is **exactly** Newton-Raphson, for other binary models it is a modification called Fisher scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inference\n",
    "\n",
    "One reason for introducing the IRLS procedure is to see what the approximate\n",
    "limiting distribution is.\n",
    "\n",
    "* The IRLS procedure suggests using approximation $\\widehat{\\beta} \\approx N(\\beta, (X'WX)^{-1})$\n",
    "* This allows us to construct CIs, test linear hypotheses, etc.\n",
    "* Intervals formed this way are called *Wald intervals*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "center = coef(flu.glm)['Age']\n",
    "SE = sqrt(vcov(flu.glm)['Age', 'Age'])\n",
    "U = center + SE * qnorm(0.975)\n",
    "L = center + SE * qnorm(0.025)\n",
    "data.frame(L, center, U)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These are slightly different from what `R` will give you if you ask it for \n",
    "confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "confint(flu.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Justifying Wald intervals\n",
    "\n",
    "- Stationarity conditions for logistic regression\n",
    "$$\n",
    "X^T(y - \\pi(\\hat{\\beta})) = 0\n",
    "$$\n",
    "\n",
    "- Taylor expansion\n",
    "$$\n",
    "\\pi(\\hat{\\beta}) = \\pi(\\beta^*) + W(\\beta^*)X(\\hat{\\beta}-\\beta^*) + R\n",
    "$$\n",
    "with $W(\\beta^*) = \\text{diag}(\\pi_i(\\beta^*)(1-\\pi_i(\\beta^*)))$.\n",
    "\n",
    "- Substituting and ignoring remainder $R$ (Why?)\n",
    "$$\n",
    "X^T(y-\\pi(\\beta^*)) \\approx X^TW(\\beta^*)X(\\hat{\\beta}-\\beta^*).\n",
    "$$\n",
    "\n",
    "- Or,\n",
    "$$\n",
    "\\hat{\\beta} \\approx \\beta^* + (X^TW(\\beta^*)X)^{-1}X^T(y - \\pi(\\beta^*)).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Justifying Wald intervals\n",
    "\n",
    "- If $X_i \\overset{IID}{\\sim} F$ and logistic model is correct then,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_F(X^T(y-\\pi(\\beta^*))|X) &= 0 \\\\\n",
    "\\text{Var}_F(X^T(y-\\pi(\\beta^*))|X) &= X^TW(\\beta^*)X \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- As $n \\to \\infty$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "n^{1/2}X^T(y-\\pi(\\beta^*)) & \\to N\\left(0, E_F(X^TW(\\beta^*)X)\\right) \\\\\n",
    "(X^TW(\\hat{\\beta})X) & \\to E_F(X^TW(\\beta^*)X).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Therefore,\n",
    "$$\n",
    "n^{1/2}(\\hat{\\beta}-\\beta^*) \\to N\\left(0, (E_F(W(\\beta^*)X_iX_i^T))^{-1} \\right)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{1}{n}X^TW(\\hat{\\beta})X\n",
    "$$\n",
    "is a plug-in estimate of $E_F(W(\\beta^*)X_iX_i^T)$.\n",
    "\n",
    "-  If logistic model is not correct, then the variance is wrong. **What is the variance in this case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Testing in logistic regression\n",
    "\n",
    "What about comparing full and reduced model?\n",
    "\n",
    "- For a model ${\\cal M}$, $DEV({\\cal M})$ replaces $SSE({\\cal M})$.\n",
    "- In least squares regression, we use \n",
    "$$\\frac{1}{\\sigma^2}\\left( SSE({\\cal M}_R) - SSE({\\cal M}_F) \\right) \\sim \\chi^2_{df_R-df_F}$$\n",
    "- This is replaced with \n",
    "$$DEV({\\cal M}_R) - DEV({\\cal M}_F) \\overset{n \\rightarrow \\infty}{\\sim} \\chi^2_{df_R-df_F}$$\n",
    "- Resulting tests do not agree numerically with those coming from IRLS (Wald tests). Both are often used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "anova(glm(Shot ~ Health.Aware, data=flu.table, family=binomial()), flu.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We should compare this difference in deviance with a $\\chi^2_1$ random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "1 - pchisq(16.863,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's compare this with the Wald test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "summary(flu.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Diagnostics\n",
    "\n",
    "* Similar to least square regression, only residuals used are *deviance residuals*\n",
    "   $r_i = \\text{sign}(Y_i-\\widehat{\\pi}_i) \\sqrt{DEV(\\widehat{\\pi}_i|Y_i)}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "plot(flu.glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "influence.measures(flu.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model selection\n",
    "\n",
    "As the model is a likelihood based model, each fitted model has an AIC.\n",
    "Stepwise selection can be used easily …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "step(flu.glm, scope=list(upper= ~.^2), direction='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LASSO\n",
    "\n",
    "- Can also fit $\\ell_1$ penalized models for logistic regression\n",
    "$$\n",
    "\\hat{\\beta}_{\\lambda} = \\text{argmin} -\\frac{1}{n}\\log \\ell(\\beta) + \\lambda \\|\\beta\\|_1\n",
    "$$\n",
    "\n",
    "- Package `glmnet` solves Gaussian, logistic, Poisson, Cox proportional hazards..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "library(glmnet)\n",
    "X = model.matrix(flu.glm)\n",
    "Y = flu.table$Shot\n",
    "plot(cv.glmnet(X, Y, family='binomial'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probit model\n",
    "\n",
    "* Probit regression model: $$\\Phi^{-1}({\\mathbb{E}}(Y_i))= \\sum_{j=1}^{p} \\beta_j X_{ij}$$ where $\\Phi$ is CDF of $N(0,1)$, i.e. $\\Phi(t) = {\\tt pnorm(t)}$.\n",
    "* Complementary log-log model (cloglog): $$-log(-log({\\mathbb{E}}(Y_i)) = \\sum_{j=1}^{p} \\beta_j X_{ij}.$$\n",
    "* In logit, probit and cloglog ${\\text{Var}}(Y_i)=\\pi_i(1-\\pi_i)$ but the model for the mean is different.\n",
    "* Coefficients no longer have an odds ratio interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "summary(glm(Shot ~ Age + Health.Aware, data=flu.table, \n",
    "            family=binomial(link='probit')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalized linear models\n",
    "\n",
    "Given a dataset $(Y_i, X_{i1}, \\dots, X_{ip}), 1 \\leq i \\leq n$ we consider a model for the distribution of $Y|X_1, \\dots, X_p$.\n",
    "* If $\\eta_i=g({\\mathbb{E}}(Y_i)) = g(\\mu_i) = \\sum_{j=1}^p \\beta_j X_{ij}$ then $g$ is called the *link*\n",
    "   function for the model.\n",
    "* If ${\\text{Var}}(Y_i) = \\phi \\cdot V({\\mathbb{E}}(Y_i)) = \\phi \\cdot V(\\mu_i)$ for $\\phi > 0$ and some function $V$, then $V$ is the called *variance*\n",
    "   function for the model.\n",
    "* Canonical reference [Generalized linear models](http://www.amazon.com/Generalized-Edition-Monographs-Statistics-Probability/dp/0412317605)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Binary regression as GLM\n",
    "\n",
    "* For a logistic model, $g(\\mu)={\\text{logit}}(\\mu), \\qquad V(\\mu)=\\mu(1-\\mu).$\n",
    "* For a probit model, $g(\\mu)=\\Phi^{-1}(\\mu), \\qquad V(\\mu)=\\mu(1-\\mu).$\n",
    "* For a cloglog model, $g(\\mu)=-\\log(-\\log(\\mu)), \\qquad V(\\mu)=\\mu(1-\\mu).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
