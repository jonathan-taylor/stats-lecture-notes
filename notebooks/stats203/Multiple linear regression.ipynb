{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiple linear regression\n",
    "\n",
    "## Outline\n",
    "\n",
    "-   Specifying the model.\n",
    "\n",
    "-   Fitting the model: least squares.\n",
    "\n",
    "-   Interpretation of the coefficients.\n",
    "\n",
    "-   More on $F$-statistics.\n",
    "\n",
    "-   Matrix approach to linear regression.\n",
    "\n",
    "-   $T$-statistics revisited.\n",
    "\n",
    "-   More $F$ statistics.\n",
    "\n",
    "-   Tests involving more than one $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prostate data\n",
    "\n",
    "For more information on the [Gleason score](http://en.wikipedia.org/wiki/Gleason_Grading_System).\n",
    "\n",
    "### Variables\n",
    "\n",
    "#### Dependent variable / response\n",
    "\n",
    "- `lpsa`: (log) Prostate Specific Antigen \n",
    "\n",
    "#### Independent variables / features\n",
    "\n",
    "- `lcavol`: (log) Cancer Volume\n",
    "- `lweight`: (log) Weight\n",
    "- `age`: Patient age\n",
    "- `lbph`: (log) Vening Prostatic Hyperplasia\n",
    "- `svi`: Seminal Vesicle Invasion\n",
    "- `lcp`: (log) Capsular Penetration\n",
    "- `gleason`: Gleason score\n",
    "- `pgg45`: Percent of Gleason score 4 or 5\n",
    "- `train`: Label for test / training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "library(ElemStatLearn)\n",
    "data(prostate)\n",
    "pairs(prostate, pch=23, bg='orange',                                          \n",
    "      cex.labels=1.5) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Specifying the model\n",
    "\n",
    "- We will use variables `lcavol, lweight, age, lbph, svi, lcp` and `pgg45`.\n",
    "\n",
    "-   Rather than one predictor, we have $p=7$ predictors.\n",
    "\n",
    "-   $$Y_i = \\beta_0 + \\beta_1 X_{i1} + \\dots + \\beta_p X_{ip} + \\varepsilon_i$$\n",
    "\n",
    "### Gauss model (fixed X)\n",
    "\n",
    "-   Errors $\\varepsilon$ are assumed independent $N(0,\\sigma^2)$, as in\n",
    "    simple linear regression.\n",
    "\n",
    "-   Coefficients are called (partial) regression coefficients because\n",
    "    they “allow” for the effect of other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Geometry of Least Squares\n",
    "\n",
    "<img src=\"http://stats203.stanford.edu/figs/axes_multiple_full.svg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting the model\n",
    "\n",
    "\n",
    "-   Just as in simple linear regression, model is fit by minimizing\n",
    "    $$\\begin{aligned}\n",
    "       SSE(\\beta_0, \\dots, \\beta_p) &= \\sum_{i=1}^n(Y_i - (\\beta_0 + \\sum_{j=1}^p X_{ij} \\beta_j ))^2 \\\\\n",
    "       &= \\|Y - \\widehat{Y}(\\beta)\\|^2\n",
    "       \\end{aligned}$$\n",
    "\n",
    "-   Minimizers:\n",
    "    $\\widehat{\\beta} = (\\widehat{\\beta}_0, \\dots, \\widehat{\\beta}_p)$\n",
    "    are the “least squares estimates”: are also normally distributed as\n",
    "    in simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "prostate.lm = lm(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + pgg45, data=prostate)\n",
    "prostate.lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Estimating $\\sigma^2$\n",
    "\n",
    "-   As in simple regression\n",
    "    $$\\widehat{\\sigma}^2 = \\frac{SSE}{n-p-1} \\sim \\sigma^2 \\cdot \\frac{\\chi^2_{n-p-1}}{n-p-1}$$\n",
    "    independent of $\\widehat{\\beta}$.\n",
    "\n",
    "-   Why $\\chi^2_{n-p-1}$? Comes from dimension of plane in picture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(prostate.lm$df.resid)\n",
    "sigma.hat = sqrt(sum(resid(prostate.lm)^2) / prostate.lm$df.resid)\n",
    "sigma.hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretation of $\\beta_j$’s\n",
    "\n",
    "-   Take $\\beta_1=\\beta_{\\tt{lcavol}}$ for example. This is the amount the average `lpsa`\n",
    "    rating increases for one “unit” of increase in `lcavol`, keeping\n",
    "    everything else constant.\n",
    "\n",
    "-   We refer to this as the effect of `lcavol` *allowing\n",
    "    for* or *controlling for* the other variables.\n",
    "    \n",
    "- For example, let's take the 10th case in our data and change `lcavol` by 1 unit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "case1 = prostate[10,]\n",
    "case2 = case1\n",
    "case2['lcavol'] = case2['lcavol'] + 1\n",
    "Yhat = predict(prostate.lm, rbind(case1, case2))\n",
    "Yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our regression model says that this difference should be $\\hat{\\beta}_{\\tt lcavol}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "c(Yhat[2]-Yhat[1], coef(prostate.lm)['lcavol'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Partial regression coefficients\n",
    "\n",
    "-   The term *partial* refers to the fact that the coefficient $\\beta_j$\n",
    "    represent the partial effect of ${X}_j$ on ${Y}$, i.e. after\n",
    "    the effect of all other variables have been removed.\n",
    "\n",
    "-   Specifically,\n",
    "    $$Y_i - \\sum_{l=1, l \\neq j}^k X_{il} \\beta_l = \\beta_0 + \\beta_j X_{ij} + \\varepsilon_i.$$\n",
    "\n",
    "- The left hand side can be interpreted as the residual after regression out $\\{X_k:k \\neq j\\}$.\n",
    "\n",
    "- **The above formula indicates that $\\beta_j$ depends on which other variables are in the model \n",
    "because the residual depends on which variables are regressed out.**\n",
    "\n",
    "-   Let $e_{i,(j)}$ be the residuals from regressing ${Y}$ onto all\n",
    "    ${X}_{\\cdot}$’s except ${X}_j$, and let $X_{i,(j)}$ be the\n",
    "    residuals from regressing ${X}_j$ onto all ${X}_{\\cdot}$’s\n",
    "    except ${X}_j$.\n",
    "\n",
    "-   If we regress $e_{i,(j)}$ against $X_{i,(j)}$, the coefficient is\n",
    "    *exactly* the same as in the original model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's verify this interpretation of regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "partial_resid_lcavol = resid(lm(lcavol ~  lweight + age + lbph + svi + lcp + pgg45, data=prostate))\n",
    "partial_resid_lpsa = resid(lm(lpsa ~  lweight + age + lbph + svi + lcp + pgg45, data=prostate))\n",
    "summary(lm(partial_resid_lpsa ~ partial_resid_lcavol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goodness of fit for multiple regression\n",
    "\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "   SSE &= \\sum_{i=1}^n(Y_i - \\widehat{Y}_i)^2 \\\\\n",
    "   SSR &= \\sum_{i=1}^n(\\overline{Y} - \\widehat{Y}_i)^2 \\\\\n",
    "   SST &= \\sum_{i=1}^n(Y_i - \\overline{Y})^2 \\\\\n",
    "   SST &= SSR+SSE \\\\\n",
    "   R^2 &= \\frac{SSR}{SST}\n",
    "   \\end{aligned}$$ \n",
    "   \n",
    "   $R^2$ is called the *multiple correlation\n",
    "coefficient* of the model, or the *coefficient of multiple\n",
    "determination*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The sums of squares and $R^2$ are defined analogously\n",
    "to those in simple linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Y = prostate$lpsa\n",
    "n = length(Y)\n",
    "SST = sum((Y - mean(Y))^2)\n",
    "MST = SST / (n - 1)\n",
    "SSE = sum(resid(prostate.lm)^2)\n",
    "MSE = SSE / prostate.lm$df.residual\n",
    "SSR = SST - SSE\n",
    "MSR = SSR / (n - 1 - prostate.lm$df.residual)\n",
    "print(c(MST,MSE,MSR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Overall fit test\n",
    "\n",
    "\n",
    "-   As in simple linear regression, we measure the overall fit of\n",
    "    the regression model by\n",
    "    $$F = \\frac{MSR}{MSE} = \\frac{\\|\\overline{Y}\\cdot {1} - \\widehat{{Y}}\\|^2/p}{\\\\\n",
    "|Y - \\widehat{{Y}}\\|^2/(n-p-1)}.$$\n",
    "\n",
    "-   Under $H_0:\\beta_1 = \\dots = \\beta_p=0$, $$F \\sim F_{p, n-p-1}$$ so\n",
    "    reject $H_0$ at level $\\alpha$ if $F > F_{p,n-p-1,1-\\alpha}.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(summary(prostate.lm))\n",
    "F = MSR / MSE\n",
    "F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Geometry of Least Squares\n",
    "\n",
    "<img src=\"http://stats203.stanford.edu/figs/axes_multiple_full.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Geometry of Least Squares\n",
    "\n",
    "<img src=\"http://stats203.stanford.edu/figs/axes_multiple_reduced.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Geometry of Least Squares\n",
    "\n",
    "<img src=\"http://stats203.stanford.edu/figs/axes_multiple.svg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition behind the $F$ test (?)\n",
    "\n",
    "-   The $F$ statistic is a ratio of lengths of orthogonal vectors\n",
    "    (divided by degrees of freedom).\n",
    "\n",
    "- Let $\\mu=E(Y)=X\\beta$ be the true mean vector for $Y$.\n",
    "\n",
    "-   We can prove that our model implies (whether $H_0$ is true or not) $$\\begin{aligned}\n",
    "       \\mathbb{E}\\left(MSR\\right) &= \\sigma^2 + \\underbrace{\\|{\\mu} - \\overline{\\mu} \\cdot {1}\\|^2 / p}_{(*)} \\\\\n",
    "       \\mathbb{E}\\left(MSE\\right) &= \\sigma^2 \\\\\n",
    "       \\mu_i &= \\mathbb{E}(Y_i) = \\beta_0 + X_{i1} \\beta_1  + \\dots +  X_{ip} \\beta_p\n",
    "       \\end{aligned}$$ \n",
    "       \n",
    "- If $H_0$ is true, then $(*)=0$ and $\\mathbb{E}(MSR)=\\mathbb{E}(MSE)=\\sigma^2$ so the $F$ should not be\n",
    "too different from 1.\n",
    "\n",
    "-   If $F$ is large, it is evidence that $\\mathbb{E}\\left(MSR\\right) \\neq \\sigma^2$, i.e. $H_0$ is\n",
    "    false.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Where does (*) come from?\n",
    "\n",
    "$$\n",
    "\\newcommand{\\FM}{{\\cal F}}\n",
    "\\newcommand{\\RM}{{\\cal R}}\n",
    "$$\n",
    "Least squares regression can be expressed in terms of orthogonal projections. \n",
    "That is, \n",
    "$$\n",
    "\\hat{Y} = PY\n",
    "$$\n",
    "for some $P_{n \\times n}$ where $P=P^T$ and $P^2=P$ (this makes it an orthogonal\n",
    "projection matrix). We will call this $P_\\FM$ where $\\FM$ stands for \"full\". Recall that for\n",
    "any projection matrix  and any vector $y$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\|Py\\|^2 &= (Py)^T(Py) \\\\\n",
    "&= y^TP^TPy  \\\\\n",
    "& = y^TP^2y \\\\\n",
    "&= y^TPy.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let $P_\\RM$ denote projection onto the 1-dimensional model determined by the 1 vector. \n",
    "Note that $P_\\FM-P_\\RM$ is again a projection: it projects onto the \n",
    "orthogonal complement of the 1 vector within the $(p+1)$-dimensional full model. So, it\n",
    "is a projection onto a $p$ dimensional space.\n",
    "\n",
    "We see that \n",
    "$$\n",
    "\\begin{aligned}\n",
    "SSR &= \\|(P_\\FM-P_\\RM)Y\\|^2 \\\\\n",
    "&= Y^T(P_\\FM-P_\\RM)Y \\\\\n",
    "&= (\\mu+\\epsilon)^T(P_\\FM-P_\\RM)(\\mu+\\epsilon) \\\\\n",
    "&= \\mu^T(P_\\FM-P_\\RM)\\mu + 2 \\mu^T(P_\\FM-P_\\RM)\\epsilon + \\epsilon^T(P_\\FM-P_\\RM)\\epsilon \\\\\n",
    "&= \\|\\mu - \\bar{\\mu} \\cdot 1\\|^2 + 2 \\mu^T(P_\\FM-P_\\RM)\\epsilon + \\epsilon^T(P_\\FM-P_\\RM)\\epsilon \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now, let's take expectations. The first term is a constant, the cross term\n",
    "has expected value zero and the expected value of the final term is\n",
    "$$\n",
    "p \\cdot \\sigma^2.\n",
    "$$\n",
    "This comes from the fact that\n",
    "$$\n",
    "\\epsilon^T(P_\\FM-P_\\RM)\\epsilon = \\|(P_\\FM-P_\\RM)\\epsilon\\|^2 \\sim \\sigma^2 \\chi^2_p.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $F$-test revisited\n",
    "\n",
    "The $F$ test can be thought of as comparing two models:\n",
    "\n",
    "$$\n",
    "\\newcommand{\\FM}{{\\cal F}}\n",
    "\\newcommand{\\RM}{{\\cal R}}\n",
    "$$\n",
    "\n",
    "-   *Full (bigger) model ($\\FM$) :*\n",
    "\n",
    "    $$Y_i = \\beta_0 + \\beta_1 X_{i1} + \\dots \\beta_p X_{ip} + \\varepsilon_i$$\n",
    "\n",
    "-   *Reduced (smaller) model ($\\RM$):*\n",
    "\n",
    "    $$Y_i = \\beta_0  + \\varepsilon_i$$\n",
    "\n",
    "-   The $F$-statistic has the form\n",
    "    $$F=\\frac{(SSE(\\RM) - SSE(\\FM)) / (df_{\\RM} - df_{\\FM})}{SSE(\\FM) / df_{\\FM}}.$$\n",
    "    \n",
    "- Under $H_0: P_R\\mu=\\mu$ the $F$ statistic has distribution $F_{df_{\\RM}-df_{\\FM},df_{\\FM}}$.\n",
    "\n",
    "- **Note: the smaller model should be nested within the bigger model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Geometry of Least Squares\n",
    "\n",
    "<img src=\"http://stats203.stanford.edu/figs/axes_general.svg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix formulation\n",
    "\n",
    "\n",
    "$${ Y}_{n \\times 1} = {X}_{n \\times (p + 1)} {\\beta}_{(p+1) \\times 1} + {\\varepsilon}_{n \\times 1}$$\n",
    "\n",
    "-   ${X}$ is called the *design matrix* of the model\n",
    "\n",
    "-   ${\\varepsilon} \\sim N(0, \\sigma^2 I_{n \\times n})$ is\n",
    "    multivariate normal\n",
    "\n",
    "## $SSE$ in matrix form\n",
    "\n",
    "$$SSE(\\beta) = ({Y} - {X} {\\beta})'({Y} - {X} {\\beta}) = \\|Y-X\\beta\\|^2_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Design matrix\n",
    "\n",
    "Design matrix\n",
    "\n",
    "-   The design matrix is the $n \\times (p+1)$ matrix\n",
    "\n",
    "    $$ X =   \\begin{pmatrix}\n",
    "   1 & X_{11} & X_{12} & \\dots & X_{1,p} \\\\\n",
    "   \\vdots &   \\vdots & \\ddots & \\vdots \\\\\n",
    "   1 & X_{n1} & X_{n2} &\\dots & X_{n,p} \\\\\n",
    "   \\end{pmatrix}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = length(Y)\n",
    "attach(prostate)\n",
    "X = cbind(rep(1,n), lcavol, lweight, age, lbph, svi, lcp, pgg45)\n",
    "detach(prostate)\n",
    "colnames(X)[1] = '(Intercept)'\n",
    "head(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The matrix X is the same as formed by `R`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "head(model.matrix(prostate.lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least squares solution\n",
    "\n",
    "\n",
    "-   Normal equations\n",
    "    $$\\frac{\\partial}{\\partial \\beta_j} SSE \\biggl|_{\\beta = \\widehat{\\beta}_{}} = -2 \\left({Y\\\n",
    "} - {X} \\widehat{\\beta}_{} \\right)^T {X}_j = 0, \\qquad 0 \\leq j \\leq p.$$\n",
    "\n",
    "-   Equivalent to $$\\begin{aligned}\n",
    "       ({Y} - {X}{\\widehat{\\beta}_{}})^T{X} &= 0 \\\\\n",
    "       {\\widehat{\\beta}} &= ({X}^T{X})^{-1}{X}^T{Y}\n",
    "       \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gauss model (fixed X)\n",
    "\n",
    "-   Properties: $$\\widehat{\\beta} \\sim N(\\beta, \\sigma^2 (X^TX)^{-1}).$$\n",
    "(Follows from our usual formula for multivariate Normal.)\n",
    "\n",
    "- Estimate of $\\sigma^2$:\n",
    "$$\n",
    "\\frac{1}{\\sigma^2}\\hat{\\sigma}^2 = \\frac{1}{n-p-1} \\|Y - \\hat{Y}\\|^2 \\sim \\frac{\\chi^2_{n-p-1}}{n-p-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least squares solution\n",
    "\n",
    "Let's verify our equations for $\\hat{\\beta}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "beta = as.numeric(solve(t(X) %*% X) %*% t(X) %*% Y)\n",
    "names(beta) = colnames(X)\n",
    "print(beta)\n",
    "print(coef(prostate.lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Regression function at one point\n",
    "\n",
    "  One thing one might want to *learn* about the regression function in\n",
    "the prostate example is something about the regression function at\n",
    "some fixed values of ${X}_{1}, \\dots, {X}_{6}$, i.e. what\n",
    "can be said about\n",
    "    \n",
    "$$\n",
    "    \\begin{aligned}\n",
    "       \\beta_0 + 1.3 \\cdot \\beta_1  &+ 3.6 \\cdot \\beta_2  + 64 \\cdot \\beta_3 + \\\\\n",
    "        0.1 \\cdot \\beta_4 &+ 0.2 \\cdot \\beta_5 - 0.2 \\cdot \\beta_6 + 25 \\cdot \\beta_7  \n",
    "        \\end{aligned}$$\n",
    "\n",
    "roughly the regression function at “typical” values of the\n",
    "predictors.\n",
    "\n",
    "The expression above is equivalent to\n",
    "$$\\sum_{j=0}^7 a_j \\beta_j, \\qquad a=(1,1.3,3.6,64,0.1,0.2,-0.2,25).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confidence interval for $\\sum_{j=0}^p a_j \\beta_j$\n",
    "\n",
    "-   Suppose we want a $(1-\\alpha)\\cdot 100\\%$ CI for\n",
    "    $\\sum_{j=0}^p a_j\\beta_j$.\n",
    "\n",
    "-   Just as in simple linear regression:\n",
    "\n",
    "    $$\\sum_{j=0}^p a_j \\widehat{\\beta}_j \\pm t_{1-\\alpha/2, n-p-1} \\cdot SE\\left(\\sum_{j=0}^p a_j\\widehat{\\beta}_j\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`R` will form these coefficients for each coefficient separately when using the `confint` function. These linear combinations are of the form\n",
    "$$\n",
    "a_{\\tt lcavol} = (0,1,0,0,0,0,0,0)\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "a_{\\tt lcavol}^T\\widehat{\\beta} = \\widehat{\\beta}_1 = {\\tt coef(prostate.lm)[2]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "confint(prostate.lm, level=0.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## $T$-statistics revisited\n",
    "\n",
    "Of course, these confidence intervals are based on the standard ingredients of a\n",
    "$T$-statistic.\n",
    "\n",
    "-   Suppose we want to test $$H_0:\\sum_{j=0}^p a_j\\beta_j= h.$$ As in\n",
    "    simple linear regression, it is based on\n",
    "    $$T = \\frac{\\sum_{j=0}^p a_j \\widehat{\\beta}_j - h}{SE(\\sum_{j=0}^p a_j \\widehat{\\beta\\\n",
    "}_j)}.$$\n",
    "\n",
    "-   If $H_0$ is true, then $T \\sim t_{n-p-1}$, so we reject $H_0$ at\n",
    "    level $\\alpha$ if $$\\begin{aligned}\n",
    "       |T| &\\geq t_{1-\\alpha/2,n-p-1}, \\qquad \\text{ OR} \\\\\n",
    "       p-\\text{value} &= {\\tt 2*(1-pt(|T|, n-p-1))} \\leq \\alpha.\n",
    "       \\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`R` produces these in the `coef` table `summary` of the linear regression model. Again, each of these \n",
    "linear combinations is a vector $a$ with only one non-zero entry like $a_{\\tt lcavol}$ above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "summary(prostate.lm)$coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's do a quick calculation to remind ourselves the relationships of the variables\n",
    "in the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "T1 = 0.56954 / 0.08584\n",
    "# 90% CI for \\beta_{lcavol}\n",
    "print(c(0.56954 - qt(0.95, prostate.lm$df.resid) * 0.08584,\n",
    "       0.56954 + qt(0.95, prostate.lm$df.resid) * 0.08584))\n",
    "P1 = 2 * (1 - pt(abs(T1), prostate.lm$df.resid))\n",
    "print(c(T1,P1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These were indeed the values for `lcavol` in the `summary` table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One-sided tests\n",
    "\n",
    "-   Suppose, instead, we wanted to test the one-sided hypothesis\n",
    "    $$H_0:\\sum_{j=0}^p a_j\\beta_j \\leq  h, \\  \\text{vs.} \\ H_a: \\sum_{j=0}^p a_j\\beta_j > \\\n",
    " h$$\n",
    "\n",
    "-   If $H_0$ is true, then $T$ is no longer exactly $t_{n-p-1}$ but we still have\n",
    "    $$\\mathbb{P}\\left(T > t_{1-\\alpha, n-p-1}\\right) \\leq 1 - \\alpha$$\n",
    "    so we reject $H_0$ at level $\\alpha$ if $$\\begin{aligned}\n",
    "       T &\\geq t_{1-\\alpha,n-p-1}, \\qquad \\text{ OR} \\\\\n",
    "       p-\\text{value} &= {\\tt (1-pt(T, n-p-1))} \\leq \\alpha.\n",
    "       \\end{aligned}$$\n",
    "       \n",
    "- **Note: the decision to do a one-sided $T$ test should be made *before* looking at the $T$ statistic. Otherwise, the probability of a false positive is doubled!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard error of $\\sum_{j=0}^p a_j \\widehat{\\beta}_j$\n",
    "\n",
    "- In order to form these $T$ statistics, we need the $SE$ of our estimate $\\sum_{j=0}^p a_j \\widehat{\\beta}_j$.\n",
    "\n",
    "-   Based on matrix approach to regression\n",
    "    $$SE\\left(\\sum_{j=0}^p a_j\\widehat{\\beta}_j \\right) = \\sqrt{\\widehat{\\sigma}^2 a^T (X^TX\\\n",
    ")^{-1} a}.$$\n",
    "\n",
    "-   Don’t worry too much about specific implementation – for much of the effects\n",
    "    we want `R` will do this for you in\n",
    "    general.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Y.hat = X %*% beta\n",
    "sigma.hat = sqrt(sum((Y - Y.hat)^2) / (n - ncol(X)))\n",
    "cov.beta = sigma.hat^2 * solve(t(X) %*% X)\n",
    "cov.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The standard errors of each coefficient estimate are the square root of the diagonal entries. They appear as the\n",
    "`Std. Error` column in the `coef` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sqrt(diag(cov.beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generally, we can find our estimate of the covariance function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vcov(prostate.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction interval\n",
    "\n",
    "-   Basically identical to simple linear regression.\n",
    "\n",
    "-   Prediction interval at $X_{1,new}, \\dots, X_{p,new}$:\n",
    "    $$\\begin{aligned}\n",
    "       \\widehat{\\beta}_0 + \\sum_{j=1}^p X_{j,new} \\widehat{\\beta}_j\\pm t_{1-\\alpha/2, n-p-1} \\times \\ \\sqrt{\\widehat{\\sigma}^2 + SE\\left(\\widehat{\\beta}_0 + \\sum_{j=1}^p X_{j,new}\\widehat{\\beta}_j\\right)^2}.\n",
    "       \\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forming intervals by hand\n",
    "\n",
    "While `R` computes most of the intervals we need,\n",
    "we could write a function that\n",
    "explicitly computes a confidence interval\n",
    "(and can be used for predicition intervals \n",
    "with the \"extra\" argument).\n",
    "\n",
    "This exercise shows the calculations that R \n",
    "is doing under the hood: the function *predict*\n",
    "is generally going to be fine for our\n",
    "purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "CI.lm = function(cur.lm, a, level=0.95, extra=0) {\n",
    "\n",
    "     # the center of the confidence interval\n",
    "     center = sum(a*cur.lm$coef)\n",
    "\n",
    "     # the estimate of sigma^2\n",
    "     sigma.sq = sum(resid(cur.lm)^2) / cur.lm$df.resid\n",
    "\n",
    "     # the standard error of sum(a*cur.lm$coef)\n",
    "     se = sqrt(extra * sigma.sq + sum((a %*% vcov(cur.lm)) * a))\n",
    "\n",
    "     # the degrees of freedom for the t-statistic\n",
    "     df = cur.lm$df\n",
    "     \n",
    "     # the quantile used in the confidence interval\n",
    "\n",
    "     q = qt((1 - level)/2, df, lower.tail=FALSE)\n",
    "\n",
    "     # upper, lower limits\n",
    "     upr = center + se * q\n",
    "     lwr = center - se * q\n",
    "     return(data.frame(center, lwr, upr))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(CI.lm(prostate.lm, c(1, 1.3, 3.6, 64, 0.1, 0.2, -0.2, 25)))\n",
    "predict(prostate.lm,\n",
    "        list(lcavol=1.3,\n",
    "             lweight=3.6,age=64,lbph=0.1,svi=0.2,lcp=-0.2,pgg45=25), \n",
    "        interval='confidence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prediction intervals\n",
    "\n",
    "By using the *extra* argument, we can make\n",
    "prediction intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(CI.lm(prostate.lm, c(1, 1.3, 3.6, 64, 0.1, 0.2, -0.2, 25), extra=1))\n",
    "predict(prostate.lm,list(lcavol=1.3,lweight=3.6,age=64,lbph=0.1,svi=0.2,lcp=-0.2,pgg45=25), \n",
    "        interval='prediction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arbitrary contrasts\n",
    "\n",
    "If we want, we can set the intercept term to 0 in `CI.lm`. This allows us to construct confidence interval for, say, how much the `lpsa` score will change will increase if we change `age` by 2 years and `svi` by 0.5 units, leaving everything else unchanged. \n",
    "\n",
    "Therefore, what we want is a confidence interval for 2 times the coefficient of `age` + 0.5 times the coefficient of `lbph`:\n",
    "$$\n",
    "2 \\cdot \\beta_{\\tt age} + 0.5  \\cdot \\beta_{\\tt svi}\n",
    "$$\n",
    "\n",
    "Most of the time, *predict* will do what you want so this \n",
    "won't be used too often.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "CI.lm(prostate.lm, c(0,0,0,2,0,0.5,0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions about many (combinations) of $\\beta_j$’s\n",
    "\n",
    "-   In multiple regression we can ask more complicated questions than in\n",
    "    simple regression.\n",
    "\n",
    "-   For instance, we could ask whether `lcp` and `pgg45` \n",
    "explains little of the variability in the data, and might be dropped\n",
    "from the regression model.\n",
    "\n",
    "-   These questions can be answered answered by $F$-statistics.\n",
    "\n",
    "- **Note: This hypothesis should really be formed *before* looking at the output of\n",
    "`summary`.**\n",
    "\n",
    "- Later we'll see some examples of the messiness when forming a hypothesis after seeing\n",
    "the `summary`...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropping one or more variables\n",
    "\n",
    "-   Suppose we wanted to test the hypothesis above.\n",
    "    Formally, the null hypothesis is: $$ H_0: \\beta_{\\tt lcp} (=\\beta_6) =\\beta_{\\tt pgg45} (=\\beta_7) =0$$\n",
    "    and the alternative is\n",
    "    $$\n",
    "    H_a = \\text{one of $ \\beta_{\\tt lcp},\\beta_{\\tt pgg45}$ is not 0}. \n",
    "    $$\n",
    "\n",
    "-   We can formulate this as an $F$-test based on two models $$\\begin{aligned}\n",
    "       \\FM: Y_i &= \\beta_0 + \\sum_{j=1}^7  X_{ij} \\beta_j + \\varepsilon_i \\\\\n",
    "       \\RM: Y_i &= \\beta_0 + \\sum_{j=1}^5 \\beta_j X_{ij} + \\varepsilon_i \\\\\n",
    "       \\end{aligned}$$\n",
    "\n",
    "-   **Note:    The reduced model $\\RM$ must be a special case of the full model $\\FM$\n",
    "    to use the $F$-test. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Geometry of Least Squares\n",
    "\n",
    "<img src=\"http://stats203.stanford.edu/figs/axes_general.svg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $SSE$ of a model\n",
    "\n",
    "-   In the graphic, a “model”, ${\\cal M}$ is a subspace of\n",
    "    $\\mathbb{R}^n$ = column space of ${X}$.\n",
    "\n",
    "-   Least squares fit = projection onto the subspace of ${\\cal M}$,\n",
    "    yielding predicted values $\\widehat{Y}_{{\\cal M}}=P_{\\cal M}Y$\n",
    "\n",
    "-   Error sum of squares:\n",
    "    $$SSE({\\cal M}) = \\|Y - \\widehat{Y}_{{\\cal M}}\\|^2 = \\|(I-P_{\\cal M})Y\\|^2.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## $F$-statistic for $H_0:\\beta_{\\tt lcp}=\\beta_{\\tt pgg45}=0$\n",
    "\n",
    "-   We compute the $F$ statistic the same to compare any models\n",
    "     $$\\begin{aligned}\n",
    "       F &=\\frac{\\frac{SSE(\\RM) - SSE(\\FM)}{2}}{\\frac{SSE(\\FM)}{n-1-p}} \\\\\n",
    "       & \\sim F_{2, n-p-1}       \\qquad   (\\text{if $H_0$ is true})\n",
    "       \\end{aligned}$$\n",
    "\n",
    "-   Reject $H_0$ at level $\\alpha$ if $F > F_{1-\\alpha, 2, n-1-p}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When comparing two models, one a special case of the other (i.e. \n",
    "one nested in the other), we can test if the smaller\n",
    "model (the special case) is roughly as good as the \n",
    "larger model in describing our outcome. This is typically\n",
    "tested using an *F* test based on comparing\n",
    "the two models. The following function does this.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f.test.lm = function(R.lm, F.lm) {\n",
    "    SSE.R = sum(resid(R.lm)^2)\n",
    "    SSE.F = sum(resid(F.lm)^2)\n",
    "    df.num = R.lm$df - F.lm$df\n",
    "    df.den = F.lm$df\n",
    "    F = ((SSE.R - SSE.F) / df.num) / (SSE.F / df.den)\n",
    "    p.value = 1 - pf(F, df.num, df.den)\n",
    "    return(data.frame(F, df.num, df.den, p.value))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`R` has a function that does essentially the same thing as `f.test.lm`: the function is \n",
    "`anova`. It can be used several ways, but it can be used to compare two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "reduced.lm = lm(lpsa ~ lcavol + lweight + age + lbph + svi, data=prostate)\n",
    "print(f.test.lm(reduced.lm, prostate.lm))\n",
    "simpler.lm = lm(lpsa ~ lcavol + lweight, data=prostate)\n",
    "anova(simpler.lm, reduced.lm, prostate.lm)\n",
    "anova(prostate.lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(1.32 / 2) / (43.107 / 89)\n",
    "(7.30 / 3) / (43.107 / 89)\n",
    "1 - pf(1.37, 2, 89)\n",
    "summary(prostate.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropping an arbitrary subset\n",
    "\n",
    "- For an arbitrary model, suppose we want to test\n",
    "   $$    \\begin{aligned}\n",
    "   H_0:&\\beta_{i_1}=\\dots=\\beta_{i_j}=0 \\\\\n",
    "   H_a:& \\text{one or more of $\\beta_{i_1}, \\dots \\beta_{i_j} \\neq 0$}\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   for some subset $\\{i_1, \\dots, i_j\\} \\subset \\{0, \\dots, p\\}$.\n",
    "   \n",
    "-   You guessed it: it is based on the two models: $$\\begin{aligned}\n",
    "       R: Y_i &= \\sum_{l=0, l \\not \\in \\{i_1, \\dots, i_j\\}}^p \\beta_j X_{il} + \\varepsilon_i \\\\\n",
    "       F: Y_i &=  \\sum_{j=0}^p \\beta_j X_{il} + \\varepsilon_i \\\\\n",
    "       \\end{aligned}$$ where $X_{i0}=1$ for all $i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - The test statistic is $$\n",
    "   \\frac{\\frac{SSE(\\RM) - SSE(\\FM)}{j}}{\\frac{SSE(F)}{n-p-1}} \\sim F_{j, n-p-1}     \\qquad    (\\text{if $H_0$ is true})\n",
    "   $$\n",
    "   \n",
    " - Reject $H_0$ at level $\\alpha$ if $F > F_{1-\\alpha, j, n-1-p}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General $F$-tests\n",
    "\n",
    "-   Given two models $\\RM \\subset \\FM$ (i.e. $\\RM$ is a subspace of $\\FM$), we\n",
    "    can consider testing $$  H_0: \\text{$\\RM$ is adequate (i.e. $\\mathbb{E}(Y) \\in \\RM$)} $$ vs. $$ H_a: \\text{$\\FM$ is adequate (i.e. $\\mathbb{E}(Y) \\in \\FM$)}\n",
    "    $$\n",
    "    \n",
    "    - The test statistic is $$  F = \\frac{(SSE(\\RM) - SSE(\\FM)) / (df_\\RM - df_\\FM)}{SSE(\\FM)/df_\\FM} $$\n",
    "\n",
    "-   If $H_0$ is true, $F \\sim F_{df_\\RM-df_\\FM, df_\\FM}$ so we reject $H_0$ at\n",
    "    level $\\alpha$ if $F > F_{df_\\RM-df_\\FM, df_\\FM, 1-\\alpha}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Constraining $\\beta_{\\tt lcavol}=\\beta_{\\tt svi}$ \n",
    "\n",
    "In this example, we might suppose that the\n",
    "coefficients for `lcavol` and `svi` are the same\n",
    "and want to test this. We do this, again, by\n",
    "comparing a \"full model\" and a \"reduced model\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-   Full model:\n",
    "    $$\\begin{aligned}\n",
    "    Y_i &= \\beta_0 + \\beta_1 X_{i,{\\tt lcavol}}  + \\beta_2 X_{i,{\\tt lweight}} + \\beta_3 X_{i, {\\tt age}} \\\\\n",
    "    & \\qquad+ \\beta_4 X_{i,{\\tt lbph}} + \\beta_5 X_{i, {\\tt svi}} + \\beta_6  X_{i, {\\tt lcp}} + \\beta_7 X_{i, {\\tt pgg45}} + \\varepsilon_i\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "-   Reduced model: \n",
    "$$\\begin{aligned}\n",
    "    Y_i &= \\beta_0 + \\tilde{\\beta}_1 X_{i,{\\tt lcavol}}  + \\beta_2 X_{i,{\\tt lweight}} + \\beta_3 X_{i, {\\tt age}} \\\\\n",
    "    & \\qquad+ \\beta_4 X_{i,{\\tt lbph}} + \\tilde{\\beta}_1 X_{i, {\\tt svi}} + \\beta_6  X_{i, {\\tt lcp}} + \\beta_7 X_{i, {\\tt pgg45}} + \\varepsilon_i\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prostate$Z = prostate$lcavol + prostate$svi\n",
    "equal.lm = lm(Y ~ Z + lweight + age + lbph + lcp + pgg45, data=prostate)\n",
    "f.test.lm(equal.lm, prostate.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Constraining $\\beta_{\\tt lcavol}+\\beta_{\\tt svi}=1$ \n",
    "\n",
    "-   Full model:\n",
    "$$\\begin{aligned}\n",
    "    Y_i &= \\beta_0 + \\beta_1 X_{i,{\\tt lcavol}}  + \\beta_2 X_{i,{\\tt lweight}} + \\beta_3 X_{i, {\\tt age}} \\\\\n",
    "    & \\qquad+ \\beta_4 X_{i,{\\tt lbph}} + \\beta_5 X_{i, {\\tt svi}} + \\beta_6  X_{i, {\\tt lcp}} + \\beta_7 X_{i, {\\tt pgg45}} + \\varepsilon_i\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "-   Reduced model: \n",
    "    $$\\begin{aligned}\n",
    "    Y_i &= \\beta_0 + \\tilde{\\beta}_1 X_{i,{\\tt lcavol}}  + \\beta_2 X_{i,{\\tt lweight}} + \\beta_3 X_{i, {\\tt age}} \\\\\n",
    "    & \\qquad+ \\beta_4 X_{i,{\\tt lbph}} + (1 - \\tilde{\\beta}_1) X_{i, {\\tt svi}} + \\beta_6  X_{i, {\\tt lcp}} + \\beta_7 X_{i, {\\tt pgg45}} + \\varepsilon_i\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prostate$Z2 = prostate$lcavol - prostate$svi\n",
    "constrained.lm = lm(lpsa ~ Z2 + lweight + age + lbph + lcp + pgg45, data=prostate, offset=svi)\n",
    "anova(constrained.lm, prostate.lm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What we had to do above was subtract *X3* from *Y* on the right hand\n",
    "side of the formula. R has a way to do this called using an *offset*. What this does\n",
    "is it subtracts this vector from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General linear hypothesis\n",
    "\n",
    "An alternative version of the $F$ test can be derived that does not require\n",
    "refitting a model.\n",
    "\n",
    "Suppose we want to test $$H_0:C_{q \\times (p+1)}\\beta_{(p+1) \\times 1} = h$$ vs.\n",
    "$$\n",
    "H_a :C_{q \\times (p+1)}\\beta_{(p+1) \\times 1} \\neq h.\n",
    "$$\n",
    "\n",
    "This can be tested via an $F$ test:\n",
    "$$\n",
    "F = \\frac{(C\\hat{\\beta}-h)^T \\left(C(X^TX)^{-1}C^T \\right)^{-1} (C\\hat{\\beta}-h) / q}{SSE(\\FM) / df_\\FM} \\overset{H_0}{\\sim} F_{q, n-p-1}.\n",
    "$$\n",
    "\n",
    "**Note: we are assuming that $\\text{rank}(C(X^TX)^{-1}C^T)=q$.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's a function that implements this computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "general.linear = function(model.lm, linear_part, null_value=0) {\n",
    "    # shorthand\n",
    "    A = linear_part\n",
    "    b = null_value\n",
    "    beta.hat = coef(model.lm)\n",
    "    V = as.numeric(A %*% beta.hat - null_value)\n",
    "    invcovV = solve(A %*% vcov(model.lm) %*% t(A)) # the MSE is included in vcov\n",
    "    \n",
    "    df.num = nrow(A)\n",
    "    df.den = model.lm$df.resid\n",
    "    F = t(V) %*% invcovV %*% V / df.num\n",
    "    p.value = 1 - pf(F, df.num, df.den)\n",
    "    return(data.frame(F, df.num, df.den, p.value))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's test verify thiss work with our test for testing $\\beta_{\\tt lcp}=\\beta_{\\tt pgg45}=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = matrix(0, 2, 8)\n",
    "A[1,7] = 1\n",
    "A[2,8] = 1\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(general.linear(prostate.lm, A))\n",
    "f.test.lm(reduced.lm, prostate.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Theory for least squares estimators\n",
    "\n",
    "## Multivariate law of averages\n",
    "\n",
    "- Suppose $W_i \\overset{IID}{\\sim} G, 1 \\leq i \\leq \\infty$ are random $k$-vectors.\n",
    "\n",
    "- Then (under some mild assumptions about $F$): the *Law of Large Numbers* says\n",
    "$$\n",
    "\\bar{W}_n = \\frac{1}{n} \\sum_{i=1}^nW_i \\overset{n \\to \\infty}{\\to} E_G(W_1)\n",
    "$$\n",
    "\n",
    "- The *Central Limit Theorem* says\n",
    "$$\n",
    "n^{1/2} \\left(\\bar{W}_n - E_G(W_1) \\right) \\overset{n \\to \\infty}{\\to} N(0, Var_G(W_1)).\n",
    "$$\n",
    "(That is, the distribution of the random $k$-vector on the left converges to the distribution on the right.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distribution of least squares estimators (random X)\n",
    "\n",
    "- Suppose $(X_i,Y_i) \\overset{IID}{\\sim} G, 1 \\leq i \\leq n$ with $X \\in \\mathbb{R}^{p+1}$.\n",
    "(We assume that $X_{i,1}=1$ to handle the intercept.)\n",
    "\n",
    "- Let $X$ be the $n \\times (p+1)$ design matrix and $Y$ the $n \\times 1$ response vector and\n",
    "$$\n",
    "\\hat{\\beta} =  \\hat{\\beta}_n = (X^TX)^{-1}(X^TY).\n",
    "$$\n",
    "\n",
    "- Define \n",
    "$$\n",
    "\\beta(G) = E_G(X_1X_1^T)^{-1} E_G(X_1\\cdot Y)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\epsilon_i = \\epsilon_i(G) = Y_i - X_i^T\\beta(G).\n",
    "$$\n",
    "\n",
    "- Then, \n",
    "$$\n",
    "n^{1/2} \\left(\\hat{\\beta}_n - \\beta(G) \\right) \\overset{n \\to \\infty}{\\to} N\\left(0, E_G(X_1X_1^T)^{-1} Var_F(X_1 \\cdot \\epsilon_1) E_G(X_1X_1^T)^{-1} \\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate linear model\n",
    "\n",
    "- The multivariate linear model with $(X_i,Y_i) \\sim G$ often makes the additional assumption:\n",
    "$$\n",
    "\\def\\indep{\\perp\\!\\!\\!\\perp}\n",
    "\\epsilon_i \\indep X_i\n",
    "$$\n",
    "\n",
    "- In this case,\n",
    "$$\n",
    "E_G(X_1X_1^T)^{-1} Var_G(X_1 \\cdot \\epsilon_1) E_G(X_1X_1^T)^{-1}  = E_G(X_1X_1^T)^{-1} Var_G(\\epsilon_1)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "n^{1/2} \\left(\\hat{\\beta}_n - \\beta(G) \\right) \\overset{n \\to \\infty}{\\to} N\\left(0, Var_G(\\epsilon_1) \\cdot E_G(X_1X_1^T)^{-1}  \\right)\n",
    "$$\n",
    "\n",
    "- The distribution on the right hand side is well-approximated (as $n \\to \\infty$) by\n",
    "$$\n",
    "N(0, \\hat{\\sigma}^2 (X^TX)^{-1})\n",
    "$$\n",
    "where $\\hat{\\sigma}^2$ is the estimate of $\\sigma^2$ in the fixed $X$ Gauss model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate linear model conditional on X\n",
    "\n",
    "- The model also has a fixed $X$ (i.e. conditional) form. The Gauss model is one example of this form.\n",
    "\n",
    "- This specifies a model for the distribution $Y_i | X_i$\n",
    "$$\n",
    "Y_i | X_i = \\beta^TX_i + \\epsilon_i\n",
    "$$\n",
    "where we assume $E[\\epsilon_i|X_i] = 0$ (unbiasedness) and $Var(\\epsilon_i|X_i)=\\sigma^2$ (i.e. it does not depend on $X$). This is\n",
    "called *homoskedasticity*.\n",
    "\n",
    "- In this form, instead of specifying a joint distribution for $(X_i,Y_i)$, we specify\n",
    "the collection of distributions \n",
    "$$\\left\\{\\text{Dbn}(Y_i \\bigl \\vert X_i=x): x \\in \\mathbb{R}^{p+1}, x_1=1\\right\\}.$$\n",
    "\n",
    "- Under these assumptions (plus a little more about the $X_i$'s) the least squares estimators\n",
    "also have the same asymptotic distribution as Gaussian fixed $X$ model.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- Random $X$ requires something like $\\epsilon_i \\indep X_i$ for results from Gaussian fixed $X$ model to be used.\n",
    "\n",
    "- Fixed $X$ requires unbiasedness and homoskedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inference for multiple regression\n",
    "\n",
    "### Results below are exact for fixed X Gauss model.\n",
    "\n",
    "### Asymptotic for multivariate linear model with random X or conditional on X\n",
    "\n",
    "#### (The asymptotics makes one of those above assumptions.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
