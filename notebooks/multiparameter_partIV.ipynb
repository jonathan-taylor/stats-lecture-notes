{
 "metadata": {
  "name": "multiparameter_partIV"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "General properties"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "By this point, we've seen many examples of exponential families as well as some algorithms related to them:\n",
      "\n",
      "1. maximum likelihood estimation $\\CGF^*$ via Newton-Raphson, constrained to remain in $\\D(\\CGF)$;\n",
      "\n",
      "2. constrained MLE: the Lagrange multiplier method;\n",
      "    \n",
      "3. stochastic approximation: when the $\\CGF$ is too complicated to compute;\n",
      "    \n",
      "4. Gibbs sampler: to simulate from $\\Pp_{\\eta}$;\n",
      "    \n",
      "5. pseudo-likelihood.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "We now remind ourselves of a few general properties before considering the most \n",
      "*popular* class of exponential families: *generalized linear models*.\n",
      "   "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Likelihoods, scores, and Fisher information"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The definitions introduced for one-parameter families are readily generalized to the multiparameter situation. Indeed, we have already been using some of them. Below,\n",
      "we assume that we have sampled $Y_i \\overset{IID}{\\sim} \\Pp_{\\eta}, 1 \\leq i \\leq n.$\n",
      "\n",
      "1. The *log-likelihood* is\n",
      "$$\n",
      "\\ell(\\eta;\\yy) = \\ell(\\eta)  =  n[\\etabold^T\\bar{\\yy}\\\n",
      " -\\CGF(\\etabold)].\n",
      "$$\n",
      "\n",
      "2. The  *score function for $\\etabold$* is\n",
      "$$\n",
      "%\\label{eqn:score-etavector}\n",
      "\\nabla \\ell(\\eta)      =  n(\\bar{\\yy} - \\mubold).\n",
      "$$\n",
      "\n",
      "3. The *Fisher information for $\\etabold$*, denoted $\\ii_{\\etabold}^{(n)}$, is\n",
      "$$\n",
      "\\ii_{\\etabold}^{(n)}  \\equiv  \\EE_{\\etabold}\\left( \\nabla \\ell_{\\eta} \\nabla \\ell_{\\eta}^T\\right)  =  n\\VV_{\\eta}   = \\VV^{(n)}_{\\eta}\n",
      " =  -\\nabla^2 \\ell_{\\etabold}.\n",
      "$$\n",
      "\n",
      "4. The *score function for $\\mubold$* is\n",
      "$$\n",
      "\\frac{\\partial}{\\partial\\mubold}\\ell_{\\etabold}(\\yy) \\,\\, = \\,\\, \\left(\\frac{\\VV_{\\eta}}{n}\\right)^{-1}(\\bar{\\yy}-\\mubold).\n",
      "$$\n",
      "\n",
      "\n",
      "\n",
      "5. The *Fisher information for $\\mubold$*, denoted $\\ii_{\\etabold}^{(n)}(\\mubold)$, is\n",
      "$$\n",
      "\\ii_{\\etabold}^{(n)}(\\mubold)  =  \\VV^{-1}_{\\eta}i_{\\etabold}^{(n)}\\VV^{-1}_{\\eta}  =  n\\VV^{-1}_{\\eta}.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "By differentiating the score function for $\\eta$, we see that $\\ell_{\\etabold}(\\yy)$ is a concave function of $\\etabold$, since $\\ddot{\\ell}_{\\etabold}=-n\\VV_{\\etabold} \\leq 0$.  In other words, the density \n",
      "$$\n",
      "\\frac{d\\Pp_{\\eta}}{dm}\n",
      "$$\n",
      "is *log-concave*.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Cramer-Rao lower bound"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "For real-valued $\\zeta=h(\\etabold)$, the *Cramer-Rao lower bound* for any unbiased estimator $\\bar{\\zeta}$ is\n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\Vv_{\\eta}(\\bar{\\zeta})& \\geq \\frac{\\nabla{h}^T_{\\eta} \\VV^{-1}_{\\eta}\\nabla{h}_{\\eta}}{n}  \\\\\n",
      "& = \\nabla{h}_{\\eta}^T(\\ii_{\\etabold}^{(n)})^{-1}\\nabla{h}_{\\eta},\n",
      "\\end{aligned}\n",
      "$$\n",
      "\n",
      "For $\\zeta=H(\\mubold)$, the bound takes the form is\n",
      "$$\n",
      "\\Vv_{\\eta}(\\bar{\\zeta}) \\geq \\nabla H_{\\mu}^T[{\\ii_{\\etabold}^{(n)}(\\mubold)}]^{-1}\n",
      " \\nabla H_{\\mu}.\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Notice that $ \\nabla H_{\\mu} ^T[{\\ii_{\\etabold}^{(n)}(\\mubold)}]^{-1} \\nabla H_{\\mu} $ is the delta method estimate of variance for $\\hat{\\zeta}=H(\\hat{\\mubold})=H(\\bar{\\yy})$.  In practice the variance estimate often used is\n",
      "$$\n",
      "\\widehat{\\Vv}_{\\etabold}(\\hat{\\zeta}) = \n",
      "\\nabla h_{\\hat{\\eta}}^T (\\ii_{\\hat{\\eta}}^{(n)})^{-1}\\nabla{h}_{\\hat{\\eta}}.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "For the $\\mubold$ itself, $\\hat{\\mubold} = \\bar{\\yy}$ is unbiased. We also see that\n",
      "it achieves the CRLB:\n",
      "\\begin{eqnarray}\n",
      "\\Vv_{\\etabold}(\\hat{\\mubold}) \\overset{\\rm CRLB}{=} \\VV_{\\eta}/n  =  {i_{\\etabold}^{(n)}(\\mubold)}^{-1}\n",
      "\\end{eqnarray}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Distribution of the MLE"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The CLT based on repeated sampling asserts that\n",
      "$$\n",
      "\\hat{\\mu} \\approx N(\\mu, \\VV_{\\eta}/n).\n",
      "$$\n",
      "While the mean and variance are correct, the approximation above is in the distribution,\n",
      "i.e. the distributional sense.\n",
      "\n",
      "Applying the delta rule yields\n",
      "$$\n",
      "\\hat{\\eta} \\approx N(\\eta, \\VV_{\\eta}^{-1}/n).\n",
      "$$\n",
      "In this case, the mean and variance are not exact, nor is the distribution exactly normal."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Skewness and kurtosis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "For multiparameter families, the mean is a $p$-vector, the covariance a $p \\times p$ matrix. These are, of course, just derivatives of $\\CGF$. Higher order derivatives\n",
      "are *tensors*. \n",
      "\n",
      "For skewness, there is a $p \\times p \\times p$ tensor, or multidimensional array\n",
      "that takes three vectors as argument. That is,\n",
      "$$\n",
      "\\nabla^3 \\CGF_{\\eta}(U,V,W) \\in \\real.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Formally, the power series for $\\CGF$ can be written as\n",
      "$$\n",
      "\\CGF(\\eta + \\Delta) = \\sum_{j=0}^{\\infty} \\frac{1}{j!} \\nabla^j \\CGF_{\\eta}(\\otimes^j \\Delta).\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: skewness of a one-parameter subfamily*\n",
      "\n",
      "Suppose we consider the one-parameter subfamily\n",
      "$$\n",
      "\\left\\{\\eta=\\eta_0 + s \\cdot v, s \\in \\real, \\eta \\in \\D(\\CGF)\\right\\}.\n",
      "$$\n",
      "\n",
      "1. What is the sufficient statistic of this one-parameter subfamily (denote the \n",
      "sufficient statistic of the original family by $t(x)$).   \n",
      "\n",
      "2. Express the skewness and kurtosis of this sufficient statistic in terms of the\n",
      "derivatives of $\\CGF$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Stein's least favorable family"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Suppose we want to estimate $\\zeta=h(\\eta)=h(\\nabla \\CGF^*(\\mu))=s(\\mu)$, a real-valued\n",
      "function of $\\eta$.\n",
      "\n",
      "Stein's least favorable family is a one-parameter subfamily of the original family, determined by some hypothesized true value, say, $\\eta_0$ and the function $h$ (or, equivalently, $s$).\n",
      "\n",
      "The family is\n",
      "$$\n",
      "\\left\\{\\eta: \\eta=\\eta_0 + \\theta \\cdot \\VV_{\\eta_0}^{-1} \\nabla h_{\\eta_0}, \n",
      "\\theta \\in \\real, \\eta \\in \\D(\\CGF) \\right\\}.\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: Stein's least favorable family*\n",
      "\n",
      "1. Show that the CRLB for the above one-parameter family is the same as the\n",
      "CRLB in the full $p$-parameter family.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Deviance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Everything we saw previously about deviance remains true in the\n",
      "multiparameter case.\n",
      "\n",
      "$$\n",
      "\\begin{aligned}\n",
      "D(\\eta_1;\\eta_2) &= D(\\Pp_{\\eta_1};\\Pp_{\\eta_2})  \\\\\n",
      "&= 2 \\left[\\CGF(\\eta_2)-\\CGF(\\eta_1) - \\nabla \\CGF(\\eta_1)^T(\\eta_2-\\eta_1)\\right] \\\\\n",
      "&= 2 \\left[\\CGF^*(\\mu_1)-\\CGF^*(\\mu_2) - \\nabla \\CGF^*(\\mu_2)^T(\\mu_1-\\mu_2) \\right] \\\\\n",
      "&= \\tilde{D}(\\mu_1;\\mu_2).\n",
      "\\end{aligned}\n",
      "$$\n",
      "with $\\mu_i=\\nabla \\CGF(\\eta_i)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The deviance and Fisher information are both related to \n",
      "Taylor series expansions of $\\CGF$.\n",
      "\n",
      "$$\n",
      "\\begin{aligned}\n",
      "D(\\eta_1;\\eta_2) &= \\nabla^2 \\CGF_{\\eta_1}(\\eta_2-\\eta_1,\\eta_2-\\eta_1) + O (\\|\\eta_2-\\eta_1\\|^3) \\\\\n",
      "&= (\\eta_2-\\eta_1)^T \\ii_{\\etabold}^{(n)}(\\eta_2-\\eta_1) + O (\\|\\eta_2-\\eta_1\\|^3).\n",
      "\\end{aligned}\n",
      "$$\n",
      "where the constant in the remainder can be expressed in terms of a Lipschitz constant of\n",
      "$\\nabla^3 \\CGF$ near $\\eta_1$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: monotone mapping*\n",
      "\n",
      "Show that, in general\n",
      "$$\n",
      "(\\eta_2-\\eta_1)^T(\\mu_2-\\mu_1) = \\frac{1}{2} \\left[D(\\eta_1;\\eta_2)+D(\\eta_2;\\eta_1)\\right] \\geq 0\n",
      "$$\n",
      "with $\\mu_i = \\nabla \\CGF_{\\eta_i}$.\n",
      "\n",
      "This demonstrates that the relationship between is globally monotone, and is an example\n",
      "of the general concept of a [monotone mapping](http://www.stanford.edu/class/ee364b/lectures/monotone_slides.pdf), the canonical example being the (sub)gradient of a convex function."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Generalized linear models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A generalized linear model (GLM) is a model the conditional\n",
      "distribution \n",
      "$${\\cal Y}|{\\cal X}, {\\cal X} \\in \\real^p, {\\cal Y} \\in \\real.$$\n",
      "\n",
      "The distribution of ${\\cal Y}|{\\cal X}$ is assumed to be in some\n",
      "one parameter exponential family $\\Pp_{\\eta}$ with sufficient statistic ${\\cal Y}$.\n",
      "\n",
      "(I am using ${\\cal X, Y}$ to ensure there is no confusion with the observed data $Y \\in \\real^n, X \\in \\real^{n \\times p}$ below)\n",
      "\n",
      "Therefore,\n",
      "$$\n",
      "{\\cal Y}|{\\cal X} \\sim \\Pp_{\\eta(X)}.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The *linear* in GLM refers to the assumption that\n",
      "$$\n",
      "\\eta(x) = x^T\\beta, \\beta \\in \\real^p.\n",
      "$$\n",
      "\n",
      "The *model* refers to the assumption that we observe\n",
      "$$\n",
      "{\\cal Y}_i | {\\cal X}_i=x_i \\overset{\\text{indep}} {\\sim} \\Pp_{\\eta(x_i)}, \\qquad 1 \\leq i \\leq n,\n",
      "$$\n",
      "where the independence here is a statement on the distribution of the vector $Y=({\\cal Y}_1, \\dots, {\\cal Y}_n)$ given the\n",
      "matrix \n",
      "$$\n",
      "X_{n \\times p} = \\begin{pmatrix}\n",
      "x_1^T \\\\\n",
      "\\vdots \\\\\n",
      "x_n^T\n",
      "\\end{pmatrix}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The parameters of the GLM are $\\beta$, and the loglikelihood is\n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\ell(\\beta) &= \\sum_{i=1}^n \\eta_i(\\beta) \\cdot y_i - \\CGF(\\eta_i(\\beta)) \\\\\n",
      "&= \\sum_{i=1}^n (x_i^T\\beta) \\cdot y_i - \\CGF(x_i^T\\beta). \\\\\n",
      "\\end{aligned}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "We could write this in matrix form as\n",
      "$$\n",
      "\\ell(\\beta) = (X\\beta)^TY - \\CGF^{(n)}(X\\beta) = \\beta^T(X^TY) - \\CGF^{(n)}(X\\beta)\n",
      "$$\n",
      "where\n",
      "$$\n",
      "\\CGF^{(n)}(\\eta) = \\sum_{i=1}^n \\CGF(\\eta_i), \\eta \\in \\real^n.\n",
      "$$\n",
      "\n",
      "Written in this way, we see that the GLM is actually a $p$-parameter \n",
      "exponential family with sufficient statistic $X^TY$ and CGF $\\tilde{\\CGF}(\\beta) = \\CGF^{(n)}(X\\beta)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Let's untangle this statement a little more carefully. \n",
      "\n",
      "1. If it \n",
      "is an exponential family, it should be a collection of distributions. In this\n",
      "case it is a collection of conditional distributions for $Y|X$. These are\n",
      "distributions on $\\real^n$.\n",
      "\n",
      "2. If it is an exponential family, it should have a reference measure.\n",
      "In this case, the reference measure is the product\n",
      "$$\n",
      "\\prod_{i=1}^n m(dy_i)\n",
      "$$\n",
      "where $m$ is the reference measure of $\\Pp_{\\eta}$.\n",
      "\n",
      "3. If it is an exponential family, it should have a natural parameter and\n",
      "a sufficient statistic. In this case, the natural parameter is $\\beta$ and the\n",
      "sufficient statistic is $X^TY$.\n",
      "\n",
      "4. The fact that the $\\tilde{\\CGF}(\\beta)=\\CGF^{(n)}(X\\beta)$ follows directly from the likelihood and the modelling assumption that\n",
      "$\\eta_i=x_i^T\\beta$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Strictly speaking, this GLM is one in which we have used the *canonical link*, i.e.\n",
      "$$\n",
      "\\eta_i = x_i^T\\beta.\n",
      "$$\n",
      "\n",
      "A model with a non-canonical link has\n",
      "$$\n",
      "\\eta_i = F(x_i^T\\beta).\n",
      "$$\n",
      "We will see examples shortly."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "### *Exercise: link functions*\n",
      "\n",
      "Suppose we use a non-canonical link function. Is the collection of distributions\n",
      "$Y|X$ an exponential family with natural parameter $\\beta$?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "### *Exercise: GLM computations*\n",
      "\n",
      "1. Show that \n",
      "$$\n",
      "\\Ee_{\\beta}(X^TY) = X^T \\nabla \\CGF^{(n)}_{X\\beta}.\n",
      "$$\n",
      "\n",
      "2. Show that\n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\Vv_{\\beta}(X^TY) &= X^T \\nabla^2 \\CGF^{(n)}_{X\\beta} X \\\\\n",
      "&= X^T \\text{diag}(\\ddot \\CGF(x_i^T\\beta)) X.\n",
      "\\end{aligned}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Score equation, Fisher information for GLM"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The score equation for $\\beta$ is \n",
      "$$\n",
      "\\nabla \\ell_{\\beta} = X^T(Y-\\nabla \\CGF^{(n)}_{X\\beta}) = X^T(Y-\\mu(\\beta))\n",
      "$$\n",
      "where\n",
      "$$\n",
      "\\mu(\\beta) = \\nabla \\CGF^{(n)}_{X\\beta} \\in \\real^n.\n",
      "$$\n",
      "\n",
      "The Fisher information for $\\beta$ is \n",
      "$$\n",
      "- \\nabla^2 \\ell_{\\beta} = X^T \\text{diag}(\\ddot \\CGF(x_i^T\\beta)) X.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This diagonal matrix can be thought of as assigning a weight to each case, so we define\n",
      "$$\n",
      "W_{\\beta} = \\text{diag}(\\ddot \\CGF(x_i^T\\beta))\n",
      "$$\n",
      "and the Fisher information has the form\n",
      "$$\n",
      "-\\nabla^2 \\ell_{\\beta} = \\ii^{(n)}_{\\beta} =  X^TW_{\\beta}X.\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Asymptotic distribution of MLE"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The general asymptotic picture for exponential families yields\n",
      "$$\n",
      "\\hat{\\beta} \\approx N(\\beta, (\\ii^{(n)}_{\\beta})^{-1}) = N(\\beta, (X^TW_{\\beta}X)^{-1}).\n",
      "$$\n",
      "Above, both the mean and variance are approximate, as is the distribution itself.\n",
      "\n",
      "The asymptotics above are, strictly speaking, for $p$ fixed and $n \\rightarrow \\infty$.\n",
      "After estimation of the MLE, this distribution is usually approximated as\n",
      "$$\n",
      "N(\\beta, X^TW_{\\hat{\\beta}}X).\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Estimation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Most generalized linear models are such that $\\tilde{\\CGF}(\\beta)=\\real^p$ so unconstrained Newton-Raphson can be used.\n",
      "\n",
      "The algorithm looks like\n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\hat{\\beta}^{(k+1)} &= \\hat{\\beta}^{(k)} - \\nabla^2 \\tilde{\\CGF}\\left(\\hat{\\beta}^{(k)}\\right)^{-1} \\left[ \\nabla \\tilde{\\CGF}\\left(\\hat{\\beta}^{(k)} \\right) - X^TY \\right] \\\\\n",
      "&= \\hat{\\beta}^{(k)} - (X^TW_{\\hat{\\beta}^{(k)}}X)^{-1} \\left[X^T \\left(\\mu(\\hat{\\beta}^{(k)})-Y \\right) \\right]\n",
      "\\end{aligned}\n",
      "$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "MLE map"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "So far, for all of our exponential families, we have written the MLE map \n",
      "in terms of the Fenchel-Legendre transform of a CGF. The GLM should be no different.\n",
      "\n",
      "The mean value space is $\\nabla \\tilde{\\CGF}(\\real^p) \\subset \\real^p$ (I am assuming the\n",
      "$\\D(\\CGF)=\\real$ here), and the MLE map should be a map from\n",
      "$\\Mm$, the convex hull of $\\nabla \\tilde{\\CGF}(\\real^p)$ back to $\\real^p$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "To find the Fenchel-Legendre transform, we would have to find\n",
      "$$\n",
      "\\tilde{\\CGF}^*(\\xi) = \\sup_{\\beta} \\xi^T\\beta - \\tilde{\\CGF}(\\beta).\n",
      "$$\n",
      "Or, we would solve\n",
      "$$\n",
      "\\minimize_{\\beta} \\tilde{\\CGF}(\\beta) - \\xi^T\\beta.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "In the GLM case, it is somewhat easier to use a dual function rather than\n",
      "the $\\tilde{\\CGF}^*$ itself, because we have an implicit constraint, namely\n",
      "$$\n",
      "\\eta_i=x_i^T\\beta.$$\n",
      "\n",
      "Our problem could then be stated formally as\n",
      "$$\n",
      "\\minimize_{\\eta, \\beta: \\eta=X\\beta} \\CGF^{(n)}(\\eta) - \\eta^TY \\equiv\n",
      "\\minimize_{\\eta, \\beta: \\eta=X\\beta} \\tilde{\\CGF}(\\beta) - (X\\beta)^TY\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Above, we have used \n",
      "$$\n",
      "\\tilde{\\CGF}(\\beta) = \\CGF^{(n)}(X\\beta) = \\sum_{i=1}^n \\CGF(x_i^T\\beta).\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "As we did in the one parameter case, let's form the Lagrangian\n",
      "$$\n",
      "\\begin{aligned}\n",
      "L(\\eta,\\beta;r) \n",
      "&= \\eta^TY - \\CGF^{(n)}(\\eta) + r^T(X\\beta-\\eta) \\\\\n",
      "&= \\sum_{i=1}^n\\left[\\eta_iy_i - \\CGF(\\eta_i) + r_i(x_i^T\\beta-\\eta_i)\\right] \\\\\n",
      "&= L_1(\\eta;r) + L_2(\\beta;r)\n",
      "\\end{aligned}\n",
      "$$\n",
      "where we have grouped together terms involving $\\eta$ into $L_1(\\eta;r)$ and those involving $\\beta$ into $L_2(\\beta;r)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Specifically,\n",
      "$$\n",
      "\\begin{aligned}\n",
      "L_1(\\eta;r) &= \\eta^T(Y-r) - \\CGF^{(n)}(\\eta)\\\\\n",
      "L_2(\\beta;r) &= r^TX\\beta \\\\\n",
      "\\end{aligned}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Minimizing  $L_1(\\eta;r)$ over  $\\eta$ yields\n",
      "$$\n",
      "-\\sum_{i=1}^n \\CGF^*(y_i-r_i)\n",
      "$$\n",
      "while minimizing $L_2(\\beta;r)$ over $\\beta$ yields a constraint \n",
      "$$\n",
      "\\sum_{i=1}^n r_ix_i^T = 0.\n",
      "$$\n",
      "Or, $r \\in \\text{null}(X^T)$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Dual problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This is the general dual function\n",
      "$$\n",
      "G(r) = -\\inf_{\\eta}L_1(\\eta;r) - \\inf_{\\beta}L_2(\\beta;r)\n",
      "$$\n",
      "and dual problem\n",
      "$$\n",
      "\\minimize_r G(r) \\equiv \\minimize_{r:X^Tr=0} \\sum_{i=1}^n \\CGF^*(y_i-r_i).\n",
      "$$\n",
      "\n",
      "Having found $\\hat{r}$, the same calculation we saw for the Lagrange multipliers\n",
      "in the one-parameter case tell us that\n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\hat{\\eta}_i &= \\dot\\CGF^*(y_i-\\hat{r}_i) \\\\\n",
      "&= X\\hat{\\beta}_i.\n",
      "\\end{aligned}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "In vector form, this reads as\n",
      "$$\n",
      "\\hat{\\eta} = \\nabla \\left(\\CGF^{(n)}\\right)^*(Y - \\hat{r}).\n",
      "$$\n",
      "\n",
      "This yields\n",
      "$$\n",
      "\\hat{\\beta} = X^{\\dagger}\\left[\\left(\\nabla{\\CGF}^{(n)} \\right)^*(Y-\\hat{r}) \\right].\n",
      "$$\n",
      "where $X^{\\dagger}$ is the pseudoinverse with\n",
      "$$\n",
      "X^{\\dagger} = (X^TX)^{-1}X^T\n",
      "$$\n",
      "when $X^TX$ is non-singular."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: variables in the dual problem*\n",
      "\n",
      "The notation $r$ for the variables in the dual problem suggest *residual*.\n",
      "\n",
      "1. Show that \n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\hat{r} &= Y - \\nabla \\CGF^{(n)}(X\\hat{\\beta}) \\\\\n",
      "&= Y - \\hat{\\mu}.\n",
      "\\end{aligned}\n",
      "$$\n",
      "                                "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "### *Exercise: influence function*\n",
      "\n",
      "This exercise computes the influence function of $\\hat{\\beta}$, i.e. \n",
      "$$\n",
      "\\frac{\\partial \\hat{\\beta}}{\\partial y}.\n",
      "$$\n",
      "\n",
      "1. Show that the influence function satisfies\n",
      "$$\n",
      "X^TW_{\\hat{\\beta}}X \\frac{\\partial \\hat{\\beta}}{\\partial y} = X^T.\n",
      "$$\n",
      "\n",
      "2. Suppose $n \\geq p$ and $X^TW_{\\hat{\\beta}}X$ is invertible. Conclude that\n",
      "$$\n",
      "\\frac{\\partial \\hat{\\beta}}{\\partial y} = \\left(X^TW_{\\hat{\\beta}}X\\right)^{-1}X^T.\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Projection picture"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "\n",
      "The GLM is a $p$-parameter subfamily of the $n$-parameter family\n",
      "$$\n",
      "\\prod_{i=1}^n \\Pp_{\\eta_i}.\n",
      "$$\n",
      "\n",
      "When $p > n$, it can even be the same family in the sense that\n",
      "the map\n",
      "$$\n",
      "\\beta \\mapsto X\\beta\n",
      "$$\n",
      "covers all of $\\real^n$.\n",
      "\n",
      "In this case, the MLE is not generally unique. For $p \\leq n$, the set\n",
      "$$\n",
      "\\Mm_X = \\left\\{\\nabla \\CGF^{(n)}(X \\beta), \\beta \\in \\real^p\\right\\} = \\left\\{\\mu(\\beta): \\beta \\in \\real^p \\right\\}\n",
      "$$\n",
      "describes a curved subset of $\\real^n$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The score equation for $\\beta$ is\n",
      "$$\n",
      "X^T(Y - \\mu(\\hat{\\beta}))=0.\n",
      "$$\n",
      "Brad expresses this by saying that the MLE projects $Y$ onto $\\Mm_X$ \"orthogonal\" to the columns of $X$.\n",
      "\n",
      "This same score equation almost arises when we consider the least squares\n",
      "projection of $Y$ onto $\\Mm_X$. That is, if we try to solve the following problem\n",
      "$$\n",
      "\\minimize_{\\mu \\in \\Mm_X} \\|Y-\\mu\\|^2_2.\n",
      "$$\n",
      "\n",
      "Then, we see that critical points in $\\Mm_X$, when parameterizing\n",
      "the problem by $\\beta$ satisfy\n",
      "$$\n",
      "Y-\\mu(\\hat{\\beta}) \\perp T_{\\mu(\\hat{\\beta})}\\Mm_X\n",
      "$$\n",
      "where the last expression is the tangent space to the curved surface $\\Mm_X$ at\n",
      "$\\mu=\\mu(\\hat{\\beta})$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: tangent space computation*\n",
      "\n",
      "1. Show that the tangent space at $\\mu(\\hat{\\beta})$ is\n",
      "made up of the span of the vectors\n",
      "$$\n",
      "\\left(x_{1j} \\ddot{\\CGF}(x_1^T\\beta), \\dots,  x_{nj} \\ddot{\\CGF}(x_n^T\\beta)\\right), 1 \\leq j \\leq p.\n",
      "$$\n",
      "\n",
      "2. Write this in matrix form.\n",
      "\n",
      "3. Use this to fully write out the stationary condition for \n",
      "$$\n",
      "\\minimize_{\\mu \\in \\Mm_X} \\|Y-\\mu\\|^2_2.\n",
      "$$\n",
      "Is this the same as the MLE stationarity equation?\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Example: using Poisson regression for density estimation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "In this example, we will use Poisson regression to fit a density to the\n",
      "$t$-statistics we saw in our application of Tweedie's formula.\n",
      "Recall, we had $N=6033$ $t$-statistics for comparing a healthy \n",
      "population and to a cancerous one.\n",
      "\n",
      "Our model will be that the density has the form\n",
      "$$\n",
      "f_{\\beta}(x) = \\exp \\left(\\sum_{j=0}^7 \\beta_j x^j - \\CGF(\\beta)\\right).\n",
      "$$\n",
      "This is an 8-parameter exponential family of distributions on $\\real$ with\n",
      "Lebesgue as reference measure."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The basic approach will be to use the Poisson trick we saw in the conditioning\n",
      "section. \n",
      "\n",
      "1. We discretize $\\real$ into $K=50$ equal\n",
      "sized bins of size $\\Delta: B_i=(L_i,U_i], 1 \\leq i \\leq K$ and\n",
      "define $$Y_i = \\# \\left\\{Z_j: Z_j \\in B_i\\right\\}$$.\n",
      "                                                                \n",
      "2. We model $Y_i \\sim \\text{Poisson}(\\mu_i)$ where\n",
      "$$\n",
      "\\mu_i = N \\cdot \\int_{(L_i,U_i]} f_{\\beta}(x) \\;dx \\approx e^{\\alpha} \\exp( \\sum_{j=1}^7\\beta_j M_i^j ) = h(M_i)\n",
      "$$                                                                \n",
      "where $N$ is the number of observed $Z$'s and $M_i=(L_i+U_i)/2$ is the midpoint of the interval.\n",
      "\n",
      "3. We fit a Poisson GLM to the $Y_i$ with design matrix\n",
      "$$\n",
      "X = \\begin{pmatrix}\n",
      "1 & M_1 & M_1^2 & \\dots & M_1^7 \\\\\n",
      "1 & M_2 & M_2^2 & \\dots & M_2^7 \\\\\n",
      "\\vdots & \\vdots & \\vdots & \\dots & \\vdots \\\\\n",
      "1 & M_K & M_K^2 & \\dots & M_K^7 \\\\\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "Actually, this design matrix is highly singular. We use `R`'s `poly` function which\n",
      "produces a design matrix with the same column span but is much better conditioned.\n",
      "\n",
      "4. This yields an estimated density\n",
      "$$\n",
      "\\hat{f}_{\\beta}(x) \\propto  \\exp \\left(\\hat{\\alpha} + \\sum_{j=1}^7 \\hat{\\beta}_j x^j \\right).\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "library(sda)\n",
      "data(singh2002)\n",
      "labels = singh2002$y\n",
      "print(summary(labels))\n",
      "expression_data = singh2002$x\n",
      "tvals = c()\n",
      "for (i in 1:6033) {\n",
      "    tvals = c(tvals, t.test(expression_data[,i] ~ labels, var.equal=TRUE)$statistic)\n",
      "}\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "display_data",
       "text": [
        "Loading required package: entropy\n",
        "Loading required package: corpcor\n",
        "Loading required package: fdrtool\n",
        " cancer healthy \n",
        "     52      50 \n",
        "Warning messages:\n",
        "1: package \u2018sda\u2019 was built under R version 2.15.3 \n",
        "2: package \u2018entropy\u2019 was built under R version 2.15.3 \n",
        "3: package \u2018corpcor\u2019 was built under R version 2.15.3 \n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "As in the previous exercise, we will convert to $Z$-scores, though this has hardly\n",
      "any effect."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R -o zvals\n",
      "zvals = qnorm(pt(tvals, 100))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hist = plt.hist(zvals,bins=50)\n",
      "f = plt.gcf()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD9CAYAAABDaefJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1087b5e50>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Next, let's split the data into bins, and compute the counts and the midpoints."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "bins = c(-Inf, seq(-4,4,length=51), Inf)\n",
      "counts = c()\n",
      "for (i in 1:(length(bins)-1)) {\n",
      "    counts = c(counts, sum((zvals > bins[i]) * (zvals <= bins[i+1])))\n",
      "}\n",
      "midpoints = (bins[1:length(bins)-1] + bins[2:length(bins)])/2\n",
      "counts = counts[2:(length(counts)-1)]\n",
      "midpoints = midpoints[2:(length(midpoints)-1)]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "To fit a GLM in `R`, we use the `glm` function. The argument \n",
      "`family` says that we are using a Poisson regression model, which sets the\n",
      "family $\\Pp_{\\eta}$ above to be a Poisson family."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "density.glm = glm(counts ~ poly(midpoints,7), family=poisson(link='log'))\n",
      "print(summary(density.glm))\n",
      "X = model.matrix(density.glm)\n",
      "C = density.glm$coef\n",
      "SE = summary(density.glm)$coef[,2]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "display_data",
       "text": [
        "\n",
        "Call:\n",
        "glm(formula = counts ~ poly(midpoints, 7), family = poisson(link = \"log\"))\n",
        "\n",
        "Deviance Residuals: \n",
        "     Min        1Q    Median        3Q       Max  \n",
        "-1.79431  -0.69422   0.01274   0.61035   2.08374  \n",
        "\n",
        "Coefficients:\n",
        "                     Estimate Std. Error z value Pr(>|z|)    \n",
        "(Intercept)           3.89067    0.03436 113.241  < 2e-16 ***\n",
        "poly(midpoints, 7)1  -0.19805    0.35327  -0.561    0.575    \n",
        "poly(midpoints, 7)2 -10.89546    0.35131 -31.014  < 2e-16 ***\n",
        "poly(midpoints, 7)3  -0.14374    0.32855  -0.437    0.662    \n",
        "poly(midpoints, 7)4   1.99022    0.30931   6.434 1.24e-10 ***\n",
        "poly(midpoints, 7)5   0.01894    0.30917   0.061    0.951    \n",
        "poly(midpoints, 7)6   0.31586    0.20410   1.548    0.122    \n",
        "poly(midpoints, 7)7   0.07490    0.20382   0.367    0.713    \n",
        "---\n",
        "Signif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n",
        "\n",
        "(Dispersion parameter for poisson family taken to be 1)\n",
        "\n",
        "    Null deviance: 6707.369  on 49  degrees of freedom\n",
        "Residual deviance:   41.079  on 42  degrees of freedom\n",
        "AIC: 342.61\n",
        "\n",
        "Number of Fisher Scoring iterations: 4\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "We see that `R` produces a standard error for each parameter we estimated. \n",
      "(Where does this come from?)\n",
      "\n",
      "For completeness, we show that using the Newton-Raphson algorithm above\n",
      "yields the same estimates of coefficients."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%R -o X,counts,C,SE"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Here is a Poisson regression model in 30 or so lines (excluding comments...)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class poisson(object):\n",
      "    \n",
      "    def __init__(self, Y):\n",
      "        self.Y = Y\n",
      "        \n",
      "    def value(self, eta):\n",
      "        '''\n",
      "        .. math::\n",
      "            \n",
      "            \\Lambda^{(n)}(\\eta)-\\eta^T Y\n",
      "\n",
      "        '''\n",
      "        return (np.exp(eta) - eta*self.Y).sum() # \n",
      "    \n",
      "    def grad(self, eta):\n",
      "        '''\n",
      "        .. math::\n",
      "\n",
      "            \\dot{\\Lambda}(\\eta) - Y\n",
      "\n",
      "        '''\n",
      "        mu = np.exp(eta)\n",
      "        return mu - self.Y\n",
      "    \n",
      "    def hess(self, eta):\n",
      "        '''\n",
      "        .. math::\n",
      "            \n",
      "            \\ddot{\\Lambda}(\\eta)\n",
      "\n",
      "        '''\n",
      "        dmu_deta = np.exp(eta)\n",
      "        return dmu_deta\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class GLM(object):\n",
      "    \n",
      "    def __init__(self, Y, X, family):\n",
      "        self.Y = Y\n",
      "        self.X = X\n",
      "        self.n, self.p = X.shape\n",
      "        self.family = family(Y)\n",
      "        \n",
      "    def value(self, beta):\n",
      "        '''\n",
      "        .. math::\n",
      "            \n",
      "            \\Lambda^{(n)}(X\\beta) - (X\\beta)^TY\n",
      "        '''\n",
      "        eta = np.dot(self.X, beta)\n",
      "        return self.family.value(eta)\n",
      "    \n",
      "    def grad(self, beta):\n",
      "        '''\n",
      "        .. math::\n",
      "\n",
      "            X^T \\left(\\nabla \\CGF^{(n)}(X\\beta) - Y)\n",
      "        '''\n",
      "        eta = np.dot(self.X, beta)\n",
      "        return np.dot(self.X.T, self.family.grad(eta))\n",
      "    \n",
      "    def hess(self, beta):\n",
      "        '''\n",
      "        .. math::\n",
      "\n",
      "            X^T \\nabla^2 \\CGF^{(n)}(X\\beta)X\n",
      "\n",
      "        '''\n",
      "        eta = np.dot(self.X, beta)\n",
      "        D = self.family.hess(eta)\n",
      "        return np.dot(self.X.T, D[:,np.newaxis] * self.X)\n",
      "    \n",
      "    def constraint(self, beta):\n",
      "        '''\n",
      "        Check to see whether the constraint is satisfied or not.\n",
      "        Most GLMs will have no constraint on $\\beta$.\n",
      "        '''\n",
      "        return True"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Let's instantiate the model and verify we've computed our gradient correctly. If correct,\n",
      "the vector `C` from `R`'s solution should have gradient approximately 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = GLM(counts, X, poisson)\n",
      "model.grad(C)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "array([ 0.0000001 , -0.00000002,  0.00000003, -0.00000002,  0.00000003,\n",
        "       -0.00000002,  0.00000002, -0.00000001])"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "As for the Hessian, the diagonal diagonal of its inverse should yield the standard errors. (Why?)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "H = model.hess(C)\n",
      "print 'Hessian inverse:', np.sqrt(np.diag(np.linalg.inv(H)))\n",
      "print 'SE:', SE"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Hessian inverse: [ 0.03435823  0.35327947  0.35132316  0.32856422  0.30931663  0.30917959\n",
        "  0.20410041  0.20382491]\n",
        "SE: [ 0.03435753  0.35327081  0.35131134  0.3285515   0.30930698  0.30917085\n",
        "  0.2040974   0.20382201]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Now, let us fit the model using Newton-Raphson."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fit(model, niter=10):\n",
      "\n",
      "    beta = np.ones(model.p)\n",
      "    value = model.value(beta)\n",
      "\n",
      "    for _ in range(niter):\n",
      "        step = 1.\n",
      "        count = 0\n",
      "        while True:\n",
      "            proposed_beta = beta - step * np.dot(np.linalg.pinv(model.hess(beta)), model.grad(beta))\n",
      "            if (model.value(proposed_beta) > value \n",
      "                and model.constraint(proposed_beta)):\n",
      "                step *= 0.5\n",
      "            else:\n",
      "                break\n",
      "        beta = proposed_beta\n",
      "        value = model.value(beta)\n",
      "    return beta\n",
      "\n",
      "beta = fit(model)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "If we've converged, the gradient should be zero."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.grad(beta)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "array([ 0.,  0., -0., -0.,  0.,  0., -0., -0.])"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The coefficients are the same as `R`, reassuringly..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'R:', C\n",
      "print 'Newton-Raphson:', beta"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "R: [  3.89067037  -0.19805055 -10.89545629  -0.14374003   1.99021697\n",
        "   0.01894121   0.31586279   0.07490286]\n",
        "Newton-Raphson: [  3.89067037  -0.19805055 -10.8954563   -0.14374002   1.99021696\n",
        "   0.01894122   0.31586278   0.07490286]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Let's add our density estimate to our earlier histogram.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R -o xval,dval\n",
      "xval=seq(-4,4,length=201)\n",
      "dval=exp(predict(density.glm, list(midpoints=xval)))\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hist = plt.hist(zvals,bins=50, normed=True)\n",
      "delta = xval[1]-xval[0]\n",
      "plt.plot(xval, (dval / dval.sum()/delta), linewidth=3)\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "[<matplotlib.lines.Line2D at 0x108036e50>]"
       ]
      },
      {
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD9CAYAAAC2l2x5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10b345f90>"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Some useful extensions"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Offset vector"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Suppose we change the constraint\n",
      "$$\n",
      "\\eta = X\\beta\n",
      "$$\n",
      "to\n",
      "$$\n",
      "\\eta = a + X\\beta.\n",
      "$$\n",
      "\n",
      "The problem changes to\n",
      "$$\n",
      "\\minimize_{\\eta,\\beta:a+X\\beta=\\eta} \\CGF^{(n)}(\\eta) - \\eta^TY.\n",
      "$$\n",
      "\n",
      "In `R`, this offset can be included by adding an `offset` argument to `glm`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: adding an offset*\n",
      "\n",
      "1. How do the Newton-Raphson iterations changes for fitting the GLM with an offset `a`?\n",
      "\n",
      "2. Thinking of the prostate cancer $Z$-statistics as a perturbation of a $N(0,1)$ distribution, refit the model above to estimate this perturbation from $N(0,1)$ as a\n",
      "7th order polynomial. What offset vector does this correspond to?\n",
      "\n",
      "3. Does it change the fitted density values? Does it change the estimate of the parameters $\\hat{\\beta}$? Is this true for any offset?\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Multiple observations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "In some modelling scenarios, one observes several observations at the same $x_i$. \n",
      "\n",
      "\n",
      "For example, suppose ${\\cal X} \\in \\real^2$ is a pair of binary variables and ${\\cal Y}$ is also binary. (This would be a logistic regression model -- coming shortly).\n",
      "\n",
      "Then, for a sample of any reasonable size, one will eventually observe each of the \n",
      "4 combinations of ${\\cal X}$. The data can be summarized in a $2 \\times 2$ table:\n",
      "$$\n",
      "\\begin{array}{l|c|c|c}\n",
      "       & {\\cal X}_1=0 & {\\cal X}_1=1  \\\\ \\hline\n",
      "   {\\cal X}_2=0 & {\\cal Y}_{11} & {\\cal Y}_{12}  \\\\\n",
      "   {\\cal X}_2=1 & {\\cal Y}_{21} & {\\cal Y}_{22} \\\\ \\hline\n",
      "     \\end{array}\n",
      "$$\n",
      "where\n",
      "$$\n",
      "{\\cal Y}_{ij} = \\sum_{k: {\\cal X}_k=(i,j)} {\\cal Y}_k\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This affects the likelihood in a fairly simple way. Let \n",
      "$$\n",
      "\\text{unique}(X) = \\left\\{X{[i]}: 1 \\leq i \\leq n\\right\\}\n",
      "$$\n",
      "denote the set of observed rows of $X_{n \\times p}$.\n",
      "\n",
      "Then, we can write\n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\tilde{\\CGF}(\\beta) &= \\CGF^{(n)}(X\\beta) \\\\\n",
      "&= \\sum_{x_u \\in \\text{unique}(X)} \\CGF(x_u^T\\beta) \\cdot \\# \\left\\{i: X{\\tt [i]}=x_u\\right\\}. \\\\\n",
      "&= \\sum_{x_u \\in \\text{unique}(X)} w(x_u) \\CGF(x_u^T\\beta)  \\\\\n",
      "\\end{aligned}\n",
      "$$\n",
      "where\n",
      "$$\n",
      "w(x_u) =  \\# \\left\\{i: X{ [i]}=x_u\\right\\}. \\\\\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The negative log-likelihood then has the form\n",
      "$$\n",
      "\\sum_{x_u \\in \\text{unique}(X)} w_u \\left[\\CGF(x_u^T\\beta) - \\bar{Y}(x_u) \\right]\n",
      "$$\n",
      "with\n",
      "$$\n",
      "\\bar{Y}(x_u) = \\frac{1}{w_u} \\sum_{i: X{ [i]}=x_u} Y_i\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This has decreased the dimension of the problem from $n$ samples to $\n",
      "\\#  \\ \\text{unique}(X).$\n",
      "\n",
      "In the binary example above, it is reduced from $n$ to $4$. In `R`, this information\n",
      "can be used using the `weights` argument to `glm`. Below, our logistic regression example\n",
      "will use these weights."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This leads us to a *GLM* with case weights, which solves the corresponding problem\n",
      "$$\n",
      "\\minimize_{\\eta, \\beta:X\\beta=\\eta} \\sum_{j=1}^m \\left[w_j \\cdot \\CGF(\\eta_j) - \\eta_j^TY_j \\right].\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: adding case weights*\n",
      "\n",
      "1. Describe the Newton-Raphson steps for a `glm` with case weights.\n",
      "\n",
      "2. When the weights come from having observed ties in the $X_i$'s, does using the\n",
      "weighted version of the `GLM` result in the same estimates of $\\eta,\\beta$?\n",
      "\n",
      "3. How does this change our estimate of the variance of $\\hat{\\beta}$?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "### *Exercise: prostate cancer*\n",
      "\n",
      "In the prostate cancer example, suppose we used unequally spaced bins in our density estimate. \n",
      "\n",
      "1. Use what we have seen about weights and offset vectors to modify the \n",
      "algorithm above to fit a histogram to our $Z$-statistics for the prostate\n",
      "cancer data set.\n",
      "\n",
      "2. Try your modified algorithm out by refitting the model using bins spaced out evenly on the normal quantile scale."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Lindsey's method"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The following is a formal generalization of what we had done above\n",
      "Suppose we observe $N$ samples $(X_1, \\dots, X_N)$ \n",
      "from an exponential family of distributions on $\\Omega$. \n",
      "\n",
      "Our exponential family\n",
      "$$\n",
      "\\Pp_{\\beta}(dx) = e^{\\beta^Tt(x) - \\CGF(\\beta)} \\cdot m(dx).\n",
      "$$\n",
      "\n",
      "The score equation for $\\beta$ is, as usual \n",
      "$$\n",
      "\\nabla \\CGF(\\hat{\\beta}) = \\frac{1}{N} \\sum_{i=1}^n t(x_i).\n",
      "$$\n",
      "\n",
      "Lindsey's method, which we describe now, is useful when there is no closed form solution for $\\CGF$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Whatever $\\Omega$ is, we assume that we can partition (finely) as $\\Omega = \\cup_{k=1}^K B_k$ with each\n",
      "element having an associated center $x_k$.\n",
      "\n",
      "The data is reduced to the counts\n",
      "$$\n",
      "N_k = \\# \\cdot \\left\\{i: X_i \\in B_k\\right\\}.\n",
      "$$\n",
      "\n",
      "Standard calculations show that\n",
      "$$\n",
      "(N_1, \\dots, N_K) \\sim \\text{Mult}(N, \\pi(\\beta))\n",
      "$$\n",
      "with\n",
      "$$\n",
      "\\begin{aligned}\n",
      "\\pi_k(\\beta) &= \\int_{B_k} e^{\\beta^Tt(x) - \\CGF(\\beta)} \\; m(dx) \\\\\n",
      "& \\approx m(B_k) \\cdot e^{\\beta^Tt(x_k) - \\CGF(\\beta)}. \\\\\n",
      "\\end{aligned}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Corresponding to our multinomial model is a Poisson model\n",
      "$$\n",
      "(N_1, \\dots, N_K) \\overset{\\text{indep.}}{\\sim} \\text{Poisson}(e^{\\alpha} \\pi(\\beta))\n",
      "$$\n",
      "where $\\alpha$ is a free rate parameter.\n",
      "\n",
      "We can think of our observed multinomial as coming from the above Poisson model\n",
      "and we simply condition on the total number of samples."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Finding the MLE in the Poisson model, yields $(\\hat{\\alpha}, \\hat{\\beta})$ where\n",
      "$\\hat{\\beta}$ is the MLE for the original multinomial problem.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### *Exercise: Poisson trick*\n",
      "\n",
      "1. Show that one can explicitly solve for $\\alpha(\\beta)$ in the Poisson model\n",
      "with observations $(N_1, \\dots, N_K)$. Do you need to assume that\n",
      "$$\n",
      "\\pi_k(\\beta) \\approx m(B_k) \\cdot e^{\\beta^Tt(x_k) - \\CGF(\\beta)}?\n",
      "$$\n",
      "\n",
      "2. Compare the resulting function of $\\beta$ with the original multinomial likelihood.\n",
      "Does this depend on the same approximation above?\n",
      "\n",
      "3. Give a formula for the estimated variance of $\\hat{\\beta}$ in the Poisson model. Does this agree with the multinomial estimate of variance?\n",
      "\n",
      "4. Given more and more data, and finer and finer partitions, informally argue that\n",
      "the limiting estimator is the MLE of the original problem, i.e. with no discretization.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Logistic regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The other common form of a GLM (perhaps more common than the Poisson model) is the \n",
      "*logistic regression model*\n",
      "$$\n",
      "{\\cal Y}|{\\cal X} \\sim \\text{Bernoulli}(\\pi({\\cal X})).\n",
      "$$\n",
      "\n",
      "Optionally, one might consider the model\n",
      "$$\n",
      "{\\cal Y}|{\\cal X} \\sim \\text{Binomial}(n({\\cal X}), \\pi({\\cal X}))\n",
      "$$\n",
      "but this is equivalent to using weights for each case."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The GLM assumes an independent sample from this model (again, independence refers \n",
      "conditional independence of ${\\cal Y}_i$'s given ${\\cal X}$):\n",
      "$$\n",
      "{\\cal Y}_i | {\\cal X}_i, n_i \\sim \\text{Binomial}(n_i, \\pi({\\cal X}_i)).\n",
      "$$\n",
      "                                                    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Recall that the $\\text{Binomial}(n,\\pi)$ is a one-parameter exponential family\n",
      "with $t(Y)=Y$ and CGF\n",
      "$$\n",
      "\\CGF(\\eta) = n \\log\\left(1 + e^{\\eta}\\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "In terms of fitting these models, what is nice about the GLM world, is that \n",
      "only the family has to be changed. Above, we defined the Poisson family. For\n",
      "logistic regression we might define a Bernoulli and a Binomial family:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class binomial(object):\n",
      "    \n",
      "    def __init__(self, YN):\n",
      "        self.Y, self.N = np.asarray(YN).T\n",
      "        \n",
      "    def value(self, eta):\n",
      "        return (self.N * np.log(1.+np.exp(eta)) - eta*self.Y).sum()\n",
      "    \n",
      "    def grad(self, eta):\n",
      "        E = np.exp(eta)\n",
      "        return self.N * E / (1. + E) - self.Y\n",
      "    \n",
      "    def hess(self, eta):\n",
      "        E = np.exp(eta)\n",
      "        mu = E / (1. + E)\n",
      "        return self.N * mu * (1 - mu)\n",
      "    \n",
      "    @classmethod\n",
      "    def bernoulli(cls, Y):\n",
      "        n = np.asarray(Y).shape[0]\n",
      "        YN = np.ones((n,2))\n",
      "        return cls(YN)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Below are some examples of the success rate for field goal kicker \n",
      "[Don Cockroft](http://en.wikipedia.org/wiki/Don_Cockroft), comparing his \n",
      "success rate with the league's success rate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cockroft = np.array([[1,4],[8,27],[15,32],[22,25],[10,12]],np.float)\n",
      "league = np.array([[29,124],[176,449],[238,372],[222,306],[226,243]], np.float)\n",
      "design = np.array([[1,55],[1,45],[1,35],[1,25],[1,12]],np.float)\n",
      "\n",
      "design[:,1] = design[:,1] - 30\n",
      "Cmodel = GLM(cockroft, design, binomial)\n",
      "Cbeta = fit(Cmodel)\n",
      "Cbeta"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "array([ 0.65048232, -0.09867412])"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "As for standard errors, they are derived from the diagonal of the Hessian"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sqrt(np.diag(np.linalg.inv(Cmodel.hess(Cbeta))))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "array([ 0.25656038,  0.02428946])"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Just to be sure, let's check with `R`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R -i cockroft,design,league\n",
      "C = cockroft[,1] / cockroft[,2]\n",
      "yards_over_30 = design[,2]\n",
      "L = league[,1] / league[,2]\n",
      "print(summary(glm(C ~ yards_over_30, weights=cockroft[,2], family=binomial())))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "display_data",
       "text": [
        "\n",
        "Call:\n",
        "glm(formula = C ~ yards_over_30, family = binomial(), weights = cockroft[, \n",
        "    2])\n",
        "\n",
        "Deviance Residuals: \n",
        "       1         2         3         4         5  \n",
        " 0.58261  -0.08412  -0.79786   1.53133  -0.96155  \n",
        "\n",
        "Coefficients:\n",
        "              Estimate Std. Error z value Pr(>|z|)    \n",
        "(Intercept)    0.65048    0.25656   2.535   0.0112 *  \n",
        "yards_over_30 -0.09867    0.02429  -4.062 4.86e-05 ***\n",
        "---\n",
        "Signif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n",
        "\n",
        "(Dispersion parameter for binomial family taken to be 1)\n",
        "\n",
        "    Null deviance: 26.4758  on 4  degrees of freedom\n",
        "Residual deviance:  4.2526  on 3  degrees of freedom\n",
        "AIC: 22.796\n",
        "\n",
        "Number of Fisher Scoring iterations: 4\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Let's compare this to the league rate"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%R print(summary(glm(L ~ yards_over_30, weights=league[,2], family=binomial())))\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "display_data",
       "text": [
        "\n",
        "Call:\n",
        "glm(formula = L ~ yards_over_30, family = binomial(), weights = league[, \n",
        "    2])\n",
        "\n",
        "Deviance Residuals: \n",
        "      1        2        3        4        5  \n",
        " 0.1339  -0.4221   1.4529  -2.0152   1.2027  \n",
        "\n",
        "Coefficients:\n",
        "               Estimate Std. Error z value Pr(>|z|)    \n",
        "(Intercept)    0.826993   0.068884   12.01   <2e-16 ***\n",
        "yards_over_30 -0.081682   0.005373  -15.20   <2e-16 ***\n",
        "---\n",
        "Signif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n",
        "\n",
        "(Dispersion parameter for binomial family taken to be 1)\n",
        "\n",
        "    Null deviance: 309.9220  on 4  degrees of freedom\n",
        "Residual deviance:   7.8145  on 3  degrees of freedom\n",
        "AIC: 40.12\n",
        "\n",
        "Number of Fisher Scoring iterations: 4\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "As above, our fit agrees with `R`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Lmodel = GLM(league, design, binomial)\n",
      "Lbeta = fit(Lmodel)\n",
      "Lbeta"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "array([ 0.82699314, -0.08168239])"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sqrt(np.diag(np.linalg.inv(Lmodel.hess(Lbeta))))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "array([ 0.06888424,  0.0053735 ])"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### *Exercise: is Cockroft better or worse than the league average?*\n",
      "\n",
      "It looks like Cockroft has a lower success rate than the league average\n",
      "from 45 yards away. \n",
      "\n",
      "1. How would you test the null hypothesis of \"no difference at 45 yards\"?\n",
      "\n",
      "2. Describe two tests, one using all the data, and one using only the 45 yard data. \n",
      "\n",
      "3. Carry out these tests for the above data.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### *Exercise: undoing case weights*\n",
      "\n",
      "For the field goal data, produce a dataset with 100 cases that has the same fitted value as \n",
      "Don Cockroft's, but each case has weight 1. Refit the binomial model. Do the results\n",
      "match with our example above?\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Other link functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "So far, we have considered only the *canonical link* in our GLMs. With this choice,\n",
      "we saw $\\eta_i=x_i^T\\beta$. \n",
      "\n",
      "There are other choices, though. The usual definition of link function for a GLM is\n",
      "$\n",
      "g:{\\cal M} \\rightarrow \\real\n",
      "$ with\n",
      "$$\n",
      "g(\\mu_i)=x_i^T\\beta.\n",
      "$$\n",
      "\n",
      "The canonical link corresponds to $g=\\dot{\\CGF}^*=(\\dot{\\CGF})^{-1}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The general model could be written as\n",
      "$$\n",
      "\\minimize_{\\eta,\\beta:F(x_i^T\\beta)=\\eta_i} \\CGF^{(n)}(\\eta)-\\eta^TY\n",
      "$$\n",
      "where \n",
      "$$\n",
      "F = (g \\circ \\dot \\CGF)^{-1}.\n",
      "$$\n",
      "\n",
      "For the canonical link, we see that $F$ is the identity function as $g=\\dot{\\CGF}^{-1}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Fisher scoring"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Generally, this is a nonlinear equality constraint so the problem is non-convex, but is typically solved with (a modified) Newton-Raphson algorithm known as \n",
      "Fisher scoring.\n",
      "\n",
      "Plugging in the nonlinear constraint above yields the problem\n",
      "$$\n",
      "\\minimize_{\\beta} \\CGF^{(n)}(F(X\\beta)) - F(X\\beta)^TY.\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "By the chain rule, the gradient of what we're trying to minimize is \n",
      "$$\n",
      "X^T \\left[\\nabla F_{X\\beta} \\left(\\nabla \\CGF^{(n)}(F(X\\beta)) - Y\\right)\\right].\n",
      "$$\n",
      "where \n",
      "$$\n",
      "\\nabla F_{X\\beta} = \\text{diag}(\\dot{F}(x_i^T\\beta), 1 \\leq i \\leq n).\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The Hessian has two terms. The first has the form\n",
      "$$\n",
      "H_1(\\beta) = X^T \\left[\\nabla F_{X\\beta} \\nabla \\CGF^{(n)}(F(X\\beta))\\nabla F_{X\\beta} \\right]X\n",
      "$$\n",
      "\n",
      "While the second has the form\n",
      "$$\n",
      "H_2(\\beta) = X^T \\left[\\nabla^2 F_{X\\beta} \\nabla \\CGF^{(n)}(F(X\\beta))\\right]X\n",
      "$$\n",
      "where\n",
      "$$\n",
      "\\nabla^2 F_{X\\beta} =  \\text{diag}(\\ddot{F}(x_i^T\\beta), 1 \\leq i \\leq n).\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Fisher scoring uses the observation that\n",
      "$$\n",
      "\\Ee_{F(X\\beta)} \\left(H_2(\\beta) \\right) = 0.\n",
      "$$\n",
      "\n",
      "The updates take the form\n",
      "$$\n",
      "\\hat{\\beta}^{(k+1)} = \\hat{\\beta}^{(k)} - H_1(\\hat{\\beta}^{(k)})^{-1} \\nabla \\left[\\CGF^{(n)}(F(X\\hat{\\beta}^{(k)})) - F(X\\hat{\\beta}^{(k)})^TY \\right].\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### *Exercise: Fisher scoring*\n",
      "\n",
      "1. For $p$ fixed, with $n$ large, give an estimate of the size of the two\n",
      "terms $H_1(\\beta)$ and $H_2(\\beta)$ for $\\beta$ near $\\beta_0$. That is,\n",
      "what is their rough mean and variance?\n",
      "\n",
      "2. Does this justify the Fisher scoring algorithm, which simply ignores $H_2$?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Probit analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The *probit* model is a common example of a GLM that does not use the \n",
      "canonical link. The probit model says that\n",
      "$$\n",
      "\\pi(\\beta) = \\Phi(x_i^T\\beta) = \\int_{-\\infty}^{x_i^T\\beta} \\frac{e^{-z^2/2}}{\\sqrt{2\\pi}} \\; dz.\n",
      "$$\n",
      "\n",
      "Therefore, $g=\\Phi^{-1}$, the normal quantile function. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "### *Exercise: probit model*\n",
      "\n",
      "1. For the probit model, determine the map $F$ that relates $x_i^T\\beta$ to $\\eta_i$. Plot it. Is it convex? \n",
      "\n",
      "2. Describe the Fisher scoring steps for the probit model in full detail. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "### *Exercise: coefficient interpretation*\n",
      "\n",
      "One of the important things to consider when changing the link is the interpretation of the\n",
      "parameters $\\beta$. \n",
      "\n",
      "1. In the logistic model, show that $e^{\\beta_j}$ represents the change in *log-odds*\n",
      "of $\\pi$ when variable $x_j$ is increased by one unit.\n",
      "\n",
      "2. What interpretation can you give to $\\beta_j$ in the probit model?\n",
      "\n",
      "3. What interpretation can you give to $\\beta_j$ in the Poisson regression model with the canonical link?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Analysis of deviance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The total deviance of a GLM is a function of $\\mu$, or, equivalently $\\eta$ and is defined to be\n",
      "$$\n",
      "\\begin{aligned}\n",
      "D_+({\\cal Y},\\mu) &= \\sum_{i=1}^n\\tilde{D}({\\cal Y}_i,\\mu_i) \\\\\n",
      "&= \\tilde{D}({\\cal Y};\\mu) \\\\\n",
      "&= \\sum_{i=1}^n \\CGF^*({\\cal Y}_i) - \\CGF^*(\\mu_i) + \\dot{\\CGF}^*(\\mu_i) ({\\cal Y}_i-\\mu_i).\n",
      "\\end{aligned}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The choice is such that, for an instance of the\n",
      "GLM $(Y,X)$ if we fit a *saturated model*, i.e. $\\text{col}(X)=\\real^n$, then\n",
      "$\\hat{\\mu}=Y$ and $D_+(Y,\\hat{\\mu})=0$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "As we saw before, we can use the deviance to arrive at Hoeffding's formula\n",
      "$$\n",
      "\\frac{d\\Pp_{\\hat{\\eta}({\\cal Y})}}{d\\Pp_{\\eta}} = e^{D(\\hat{\\eta}({\\cal Y});\\eta)} = e^{D_+({\\cal Y};\\mu)}\n",
      "$$\n",
      "where \n",
      "$$\n",
      "\\hat{\\eta}({\\cal Y}) = \\nabla \\left(\\CGF^{(n)} \\right)^*({\\cal Y}).\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The total deviance replaces the error sum of squares from ordinary least squares regression. Specifically, \n",
      "$$\n",
      "D_+(Y, \\hat{\\mu}_X) = 2 \\cdot \\inf_{\\eta,\\beta:X\\beta=\\eta} \\left[\\CGF^{(n)}(\\eta) - \\eta^TY - \\left(\\CGF^{(n)} (\\hat{\\eta}(Y) - \\hat{\\eta}(Y)^TY \\right) \\right]  \n",
      "$$\n",
      "where\n",
      "$$\n",
      "\\hat{\\mu}_X = \\nabla \\CGF^{(n)}(X\\hat{\\beta})\n",
      "$$\n",
      "is the vector of fitted values for the GLM.\n",
      "\n",
      "That is, the total deviance is twice the difference between the minimized\n",
      "(negative) log-likelihood and its unrestricted minimum value."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Nested GLMs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The total deviance also is used to compare two models, one nested within\n",
      "the other.\n",
      "\n",
      "Suppose we have a GLM for which we can partition the\n",
      "design $X$ as\n",
      "$$\n",
      "X = \\begin{pmatrix} X^{(1)}_{n \\times p_1} & X^{(2)}_{n \\times p_2}\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "The full model achieves\n",
      "$\n",
      "D_+(Y, \\hat{\\mu}_X).\n",
      "$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "We might fit the reduced model with $\\beta^{(2)}\\equiv 0$. That is,\n",
      "$$\n",
      "\\minimize_{\\eta, \\beta:X^{(1)}\\beta^{(1)}=\\eta,\\beta^{(2)}=0} \\CGF^{(n)}(\\eta) - Y^T\\eta.\n",
      "$$\n",
      "This model\n",
      "achieves $D_+(Y, \\hat{\\mu}_{X^{(1)}})$.\n",
      "\n",
      "From our calculations above, we see that the difference\n",
      "$$\n",
      "D_+(Y, \\hat{\\mu}_{X^{(1)}}) - D_+(Y, \\hat{\\mu}_X)\n",
      "$$\n",
      "is twice the difference in the maximized log-likelihoods."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "By Wilks' theorem,\n",
      "we see that under $H_0:\\beta^{(2)}=0$\n",
      "$$\n",
      "D_+(Y, \\hat{\\mu}_{X^{(1)}}) - D_+(Y, \\hat{\\mu}_X) \\overset{n \\rightarrow \\infty}{\\to} \\chi^2_{p_2}.\n",
      "$$\n",
      "\n",
      "The term on the left above is actually \n",
      "$$D_+\\left(\\hat{\\mu}_X,\\hat{\\mu}_{X^{(1)}}\\right) = D_+(Y, \\hat{\\mu}_{X^{(1)}}) - D_+(Y, \\hat{\\mu}_X).$$\n",
      "This is the *additivity theorem*.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "### *Exercise: additivity theorem*\n",
      "\n",
      "Prove the additivity theorem."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Analysis of deviance table"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Consider now a partition of the design matrix into $K$ blocks of columns\n",
      "$$\n",
      "X = \\begin{pmatrix} X^{(1)}_{n \\times p_1} & \\dots & X^{(K)}_{n \\times p_K}\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "with a corresponding partition of \n",
      "$$\n",
      "   \\beta = \\begin{pmatrix} \\beta^{(1)} \\in \\real^{p_1} \\\\ \\vdots \\\\ \\beta^{(K)}  \\in \\real^{p_K}\n",
      "\\end{pmatrix}  \\in \\real^{p_1+\\dots+p_K}\n",
      "$$\n",
      "\n",
      "We consider the model \n",
      "$$\n",
      "\\eta = \\alpha + X\\beta.\n",
      "$$\n",
      "\n",
      "For $1 \\leq j \\leq K-1$ set $\\hat{\\beta}^j$ to be the MLE under \n",
      "$H_j: \\beta^{(l)}=0, j+1 \\leq l \\leq K$.\n",
      "    \n",
      "Here, the model $\\hat{\\beta}^0$ corresponds to $\\eta=\\alpha$, an intercept but\n",
      "no other covariates in the model. The deviance of this model is referred to as the \n",
      "null deviance.\n",
      "    \n",
      "  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This leads to an analysis of deviance table\n",
      "$$\n",
      "\\begin{array}{l|c|c|c}\n",
      "\\text{MLE} & \\text{deviance difference} & \\text{degrees of freedom} \\\\\n",
      "\\hline\n",
      "\\hat{\\beta}^1 & D_+(\\hat{\\mu}^{1},\\hat{\\mu}^0) & p_1 \\\\\n",
      "\\hat{\\beta}^2 & D_+(\\hat{\\mu}^{2},\\hat{\\mu}^1) & p_2 \\\\\n",
      "\\vdots & \\vdots & \\vdots \\\\\n",
      "\\hat{\\beta}^K & D_+(\\hat{\\mu}^K,\\hat{\\mu}^{K-1}) & p_K\n",
      "\\end{array}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The residual deviance of the model is\n",
      "$\n",
      "D_+(Y,\\hat{\\mu}^k).\n",
      "$\n",
      "The additivity formula now implies that\n",
      "$$\n",
      "D_+(\\hat{\\mu}^k,\\hat{\\mu}^0) = \\sum_{j=0}^{K-1} D_+(\\hat{\\mu}^{j+1},\\hat{\\mu}^j) = \n",
      "D_+(Y,\\hat{\\mu}^0) - D_+(Y,\\hat{\\mu}^k).\n",
      "$$\n",
      "\n",
      "\n",
      "This is the difference between the null deviance and *residual deviance* in `R`'s output.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "print(summary(density.glm))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "display_data",
       "text": [
        "\n",
        "Call:\n",
        "glm(formula = counts ~ poly(midpoints, 7), family = poisson(link = \"log\"))\n",
        "\n",
        "Deviance Residuals: \n",
        "     Min        1Q    Median        3Q       Max  \n",
        "-1.79431  -0.69422   0.01274   0.61035   2.08374  \n",
        "\n",
        "Coefficients:\n",
        "                     Estimate Std. Error z value Pr(>|z|)    \n",
        "(Intercept)           3.89067    0.03436 113.241  < 2e-16 ***\n",
        "poly(midpoints, 7)1  -0.19805    0.35327  -0.561    0.575    \n",
        "poly(midpoints, 7)2 -10.89546    0.35131 -31.014  < 2e-16 ***\n",
        "poly(midpoints, 7)3  -0.14374    0.32855  -0.437    0.662    \n",
        "poly(midpoints, 7)4   1.99022    0.30931   6.434 1.24e-10 ***\n",
        "poly(midpoints, 7)5   0.01894    0.30917   0.061    0.951    \n",
        "poly(midpoints, 7)6   0.31586    0.20410   1.548    0.122    \n",
        "poly(midpoints, 7)7   0.07490    0.20382   0.367    0.713    \n",
        "---\n",
        "Signif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n",
        "\n",
        "(Dispersion parameter for poisson family taken to be 1)\n",
        "\n",
        "    Null deviance: 6707.369  on 49  degrees of freedom\n",
        "Residual deviance:   41.079  on 42  degrees of freedom\n",
        "AIC: 342.61\n",
        "\n",
        "Number of Fisher Scoring iterations: 4\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "print(anova(density.glm))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "display_data",
       "text": [
        "Analysis of Deviance Table\n",
        "\n",
        "Model: poisson, link: log\n",
        "\n",
        "Response: counts\n",
        "\n",
        "Terms added sequentially (first to last)\n",
        "\n",
        "\n",
        "                   Df Deviance Resid. Df Resid. Dev\n",
        "NULL                                  49     6707.4\n",
        "poly(midpoints, 7)  7   6666.3        42       41.1\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "`R` recognizes that all the variables in its model were derived from \n",
      "`midpoints`. Here is a slightly tedious way to see the full table."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "X1 = midpoints\n",
      "X2 = midpoints^2\n",
      "X3 = midpoints^3\n",
      "X4 = midpoints^4\n",
      "X5 = midpoints^5\n",
      "X6 = midpoints^6\n",
      "X7 = midpoints^7\n",
      "print(anova(glm(counts ~ X1 + X2 + X3 + X4 + X5 + X6 + X7, family=poisson())))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "display_data",
       "text": [
        "Analysis of Deviance Table\n",
        "\n",
        "Model: poisson, link: log\n",
        "\n",
        "Response: counts\n",
        "\n",
        "Terms added sequentially (first to last)\n",
        "\n",
        "\n",
        "     Df Deviance Resid. Df Resid. Dev\n",
        "NULL                    49     6707.4\n",
        "X1    1      0.0        48     6707.4\n",
        "X2    1   6614.3        47       93.0\n",
        "X3    1      0.3        46       92.7\n",
        "X4    1     49.0        45       43.7\n",
        "X5    1      0.1        44       43.5\n",
        "X6    1      2.3        43       41.2\n",
        "X7    1      0.1        42       41.1\n"
       ]
      }
     ],
     "prompt_number": 25
    }
   ],
   "metadata": {}
  }
 ]
}