{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformations\n",
    "\n",
    "Transformations to achieve linearity\n",
    "\n",
    "-   We have been working with *linear* regression models so far in the\n",
    "    course.\n",
    "\n",
    "-   Some models are nonlinear, but can be *transformed* to a linear\n",
    "    model.\n",
    "    \n",
    "-   We will also see that transformations can sometimes *stabilize* the variance\n",
    "    making constant variance a more reasonable assumption.\n",
    "\n",
    "-   Finally, we will see how to correct for unequal variance using a technique weighted least squares (WLS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bacterial colony decay\n",
    "\n",
    "Here is a simple dataset showing the number of bacteria alive in a colony, $N_t$\n",
    "as a function of time $t$. A simple linear regression model is clearly not a very\n",
    "good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "bacteria.table = read.table('http://stats191.stanford.edu/data/bacteria.table', header=T)\n",
    "plot(bacteria.table$t, bacteria.table$N_t, pch=23, cex=2, bg='orange')\n",
    "bacteria.lm = lm(N_t ~ t, bacteria.table)\n",
    "abline(bacteria.lm$coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "par(mfrow=c(2,2))\n",
    "plot(bacteria.lm, pch=23, bg='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exponential decay model\n",
    "\n",
    "-   Suppose the expected number of cells grows like\n",
    "    $$E(n_t) = n_0 e^{\\beta_1t}, \\qquad t=1, 2, 3, \\dots$$\n",
    "\n",
    "-   If we take logs of both sides\n",
    "    $$\\log E(n_t) = \\log n_0 + \\beta_1 t.$$\n",
    "\n",
    "-   A reasonable (?) model:\n",
    "    $$\\log n_t = \\log n_0 + \\beta_1 t + \\varepsilon_t, \\qquad \\varepsilon_t \\overset{IID}{\\sim} N(0,\\sigma^2).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "bacteria.log.lm <- lm(log(N_t) ~ t, bacteria.table)\n",
    "plot(bacteria.table$t, bacteria.table$N_t, pch=23, cex=2, bg='orange')\n",
    "lines(bacteria.table$t, fitted(bacteria.lm), lwd=2, col='red')\n",
    "lines(bacteria.table$t, exp(fitted(bacteria.log.lm)), lwd=2, col='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "par(mfrow=c(2,2))\n",
    "plot(bacteria.log.lm, pch=23, bg='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logarithmic transformation\n",
    "\n",
    "-   This model slightly different than original model:\n",
    "    $$E(\\log n_t) \\leq \\log E(n_t)$$ but may be approximately true.\n",
    "\n",
    "-   If $\\varepsilon_t \\sim N(0,\\sigma^2)$ then\n",
    "    $$n_t = n_0 \\cdot \\epsilon_t \\cdot e^{\\beta_1 t}.$$\n",
    "\n",
    "-   $\\epsilon_t=e^{\\varepsilon_t}$ is called a log-normal random\n",
    "    $(0,\\sigma^2)$ random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linearizing regression function\n",
    "\n",
    "We see that an exponential growth or decay model can\n",
    "be made (approximately) linear. Here are a few other models that can be linearized:\n",
    "\n",
    "-   $y=\\alpha x^{\\beta}$, use $\\tilde{y}=\\log(y), \\tilde{x}=\\log(x)$;\n",
    "\n",
    "-   $y=\\alpha e^{\\beta x}$, use $\\tilde{y}=\\log(y)$;\n",
    "\n",
    "-   $y=x/(\\alpha x - \\beta)$, use $\\tilde{y}=1/y, \\tilde{x}=1/x$.\n",
    "\n",
    "-   More in textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Caveats\n",
    "\n",
    "-   Just because expected value linearizes, doesn’t mean that the errors\n",
    "    behave correctly.\n",
    "\n",
    "-   In some cases, this can be corrected using weighted least squares\n",
    "    (more later).\n",
    "\n",
    "-   Constant variance, normality assumptions should still be checked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stabilizing variance\n",
    "\n",
    "-   Sometimes, a transformation can turn non-constant variance errors to\n",
    "    \"close to\" constant variance. This is another situation in which we might\n",
    "    consider a transformation.\n",
    "\n",
    "-   Example: by the \"delta rule\", if\n",
    "    $$Var(Y) = \\sigma^2 E(Y)$$ then\n",
    "    $$\\text{Var}(\\sqrt{Y}) \\simeq \\frac{\\sigma^2}{4}.$$\n",
    "    \n",
    "-   In practice, we might not know which transformation is best. [Box-Cox transformations](http://en.wikipedia.org/wiki/Power_transform) offer a tool to find a \"best\" transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Delta rule\n",
    "\n",
    "The following approximation is ubiquitous in statistics.\n",
    "\n",
    "-   Taylor series expansion:\n",
    "    $$f(Y) = f(E(Y)) + \\dot{f}(E(Y)) (Y - E(Y)) + \\dots$$\n",
    "\n",
    "-   Taking expectations of both sides yields:\n",
    "    $$\\text{Var}(f(Y)) \\simeq \\dot{f}(E(Y))^2  \\cdot \\text{Var}(Y)$$\n",
    "\n",
    "-  So, for our previous example:\n",
    "    $$\\text{Var}(\\sqrt{Y}) \\simeq \\frac{\\text{Var}(Y)}{4 \\cdot E(Y)}$$\n",
    "    \n",
    "- Another example\n",
    "    $$\\text{Var}(\\log(Y)) \\simeq \\frac{\\text{Var}(Y)}{4 \\cdot E(Y)^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Caveats\n",
    "\n",
    "-   Just because a transformation makes variance constant doesn’t mean\n",
    "    regression function is still linear (or even that it was linear)!\n",
    "\n",
    "-   The models are approximations, and once a model is selected our\n",
    "    standard \"diagnostics\" should be used to assess adequacy of fit.\n",
    "\n",
    "-   It is possible to have non-constant variance but the variance\n",
    "    stabilizing transformation may destroy linearity of the regression\n",
    "    function. \n",
    "    \n",
    "    - *Solution:* try weighted least squares (WLS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Correcting for unequal variance\n",
    "\n",
    "We will now see an example in which there seems to be strong evidence\n",
    "for variance that changes based on `Region`.\n",
    "\n",
    "After observing this, we will create a new model that\n",
    "attempts to *correct* for this and come up with better estimates.\n",
    "\n",
    "*Correcting* for unequal variance, as we describe it here, generally\n",
    "requires a model for how the variance depends on observable quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from ipy_table import *\n",
    "desc = [('Variable', 'Description'),\n",
    "        ('$Y$', 'per capita education expenditure by state'),\n",
    "        ('$X_1$', 'per capita income in 1973 by state'),\n",
    "        ('$X_2$', 'proportion of population under 18'),\n",
    "        ('$X_3$', 'proportion in urban areas'),\n",
    "        ('Region', 'which region of the country are the states located in ?')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "make_table(desc)\n",
    "apply_theme('basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "education.table = read.table('http://stats191.stanford.edu/data/education1975.table', header=T)\n",
    "education.table$Region = factor(education.table$Region)\n",
    "education.lm = lm(Y ~ X1 + X2 + X3, data=education.table)\n",
    "summary(education.lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "par(mfrow=c(2,2))\n",
    "plot(education.lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "boxplot(rstandard(education.lm) ~ education.table$Region, \n",
    "        col=c('red', 'green', 'blue', 'yellow'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "keep.subset = (education.table$STATE != 'AK')\n",
    "education.noAK.lm = lm(Y ~ X1 + X2 + X3, subset=keep.subset, data=education.table)\n",
    "summary(education.noAK.lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R -w 800 -h 800\n",
    "par(mfrow=c(2,2))\n",
    "plot(education.noAK.lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "par(mfrow=c(1,1))\n",
    "boxplot(rstandard(education.noAK.lm) ~ education.table$Region[keep.subset], \n",
    "        col=c('red', 'green', 'blue', 'yellow'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reweighting observations\n",
    "\n",
    "-   If you have a reasonable guess of variance as a function of the\n",
    "    predictors, you can use this to *reweight* the data.\n",
    "\n",
    "-   Hypothetical example\n",
    "    $$Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, \\qquad \\varepsilon_i \\sim N(0,\\sigma^2 X_i^2).$$\n",
    "\n",
    "-   Setting $\\tilde{Y}_i = Y_i / X_i$, $\\tilde{X}_i = 1 / X_i$, model\n",
    "    becomes\n",
    "    $$\\tilde{Y}_i = \\beta_0 \\tilde{X}_i + \\beta_1 + \\epsilon_i, \\epsilon_i \\sim N(0,\\sigma^2).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weighted Least Squares\n",
    "\n",
    "-   Fitting this model is equivalent to minimizing\n",
    "    $$\\sum_{i=1}^n \\frac{1}{X_i^2} \\left(Y_i - \\beta_0 - \\beta_1 X_i\\right)^2$$\n",
    "\n",
    "-   Weighted Least Squares\n",
    "    $$SSE(\\beta, w) = \\sum_{i=1}^n w_i \\left(Y_i - \\beta_0 - \\beta_1 X_i\\right)^2, \\qquad w_i = \\frac{1}{X_i^2}.$$\n",
    "\n",
    "-   In general, weights should be like:\n",
    "    $$w_i = \\frac{1}{\\text{Var}(\\varepsilon_i)}.$$\n",
    "    \n",
    "- Our education expenditure example assumes\n",
    "    $$\n",
    "    w_i = W_{\\tt Region[i]}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common weighting schemes\n",
    "\n",
    "-   If you have a qualitative variable, then it is easy to estimate\n",
    "    weight within groups (our example today).\n",
    "\n",
    "-   \"Often\" $$\\text{Var}(\\varepsilon_i) = \\text{Var}(Y_i) = V(E(Y_i))$$\n",
    "\n",
    "-   Many non-Gaussian (non-Normal) models behave like this: logistic, Poisson\n",
    "    regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two stage procedure\n",
    "\n",
    "-   Suppose we have a hypothesis about the weights, i.e. they are\n",
    "    constant within Region, or they are something like\n",
    "    $$w_i^{-1} = \\text{Var}(\\epsilon_i) =  \\alpha_0 + \\alpha_1 X_{i1}^2.$$\n",
    "\n",
    "-   We pre-whiten:\n",
    "\n",
    "    1.  Fit model using OLS (Ordinary Least Squares) to get initial\n",
    "        estimate $\\widehat{\\beta}_{OLS}$\n",
    "\n",
    "    2.  Use predicted values from this model to estimate $w_i$.\n",
    "\n",
    "    3.  Refit model using WLS (Weighted Least Squares).\n",
    "\n",
    "    4.  If needed, iterate previous two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "educ.weights = 0 * education.table$Y\n",
    "for (region in levels(education.table$Region)) {\n",
    "  subset.region = (education.table$Region[keep.subset] == region)\n",
    "  educ.weights[subset.region] <- 1.0 / (sum(resid(education.noAK.lm)[subset.region]^2) / sum(subset.region))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "unique(educ.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is our new model. Note that the scale of the estimates is *unchanged*. Numerically\n",
    "the estimates are similar. What changes most is the `Std. Error` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "education.noAK.weight.lm <- lm(Y ~ X1 + X2 + X3, weights=educ.weights, subset=keep.subset, data=education.table)\n",
    "summary(education.noAK.weight.lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "summary(education.noAK.lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "par(mfrow=c(2,2))\n",
    "plot(education.noAK.weight.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the boxplot again. It looks better, but perhaps not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "par(mfrow=c(1,1))\n",
    "boxplot(resid(education.noAK.weight.lm, type='pearson') ~ education.table$Region[keep.subset],\n",
    "        col=c('red', 'green', 'blue', 'yellow'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unequal variance: effects on inference\n",
    "\n",
    "-   So far, we have just mentioned that things *may* have unequal\n",
    "    variance, but not thought about how it affects inference.\n",
    "\n",
    "-   In general, if we ignore unequal variance, our estimates of variance\n",
    "    are no longer unbiased. The covariance has the “sandwich form”\n",
    "    $$\\text{Cov}(\\widehat{\\beta}_{OLS}) = (X'X)^{-1}(XW^{-1}X)(X'X)^{-1}.$$\n",
    "    with $W=\\text{diag}(1/\\sigma^2_i)$.\n",
    "    \n",
    "- ** If our `Std. Error` is incorrect, so are our conclusions based on $t$-statistics!**\n",
    "\n",
    "- In this example, correcting for weights seemed to make the $t$-statistics larger. ** This will not always be the case!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Efficiency\n",
    "\n",
    "-   The efficiency of an unbiased estimator of $\\beta$ is 1 / variance.\n",
    "\n",
    "-   Estimators can be compared by their efficiency: the more efficient,\n",
    "    the better.\n",
    "\n",
    "-   The other reason to correct for unequal variance (besides so that we\n",
    "    get valid inference) is for efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Illustrative example\n",
    "\n",
    "-   Suppose\n",
    "    $$Z_i = \\mu + \\varepsilon_i, \\qquad \\varepsilon_i \\sim N(0, i^2 \\cdot \\sigma^2), 1 \\leq i \\leq n.$$\n",
    "\n",
    "-   Two unbiased estimators of $\\mu$: $$\\begin{aligned}\n",
    "       \\widehat{\\mu}_1 &= \\frac{1}{n}\\sum_{i=1}^n Z_i \\\\\n",
    "       \\widehat{\\mu}_2 &= \\frac{1}{\\sum_{i=1}^n i^{-2}}\\sum_{i=1}^n i^{-2}Z_i \\\\\n",
    "       \\widehat{\\mu}_3 &= \\frac{1}{\\sum_{i=1}^n i^{-1}}\\sum_{i=1}^n i^{-1}Z_i \\\\\n",
    "       \\end{aligned}$$\n",
    "\n",
    "-   The estimator $\\widehat{\\mu}_2$ will always have lower variance,\n",
    "    hence tighter confidence intervals. \n",
    "    \n",
    "- The estimator $\\widehat{\\mu}_3$ has incorrect weights, but they are \"closer\" to correct\n",
    "than the naive mean's weights which assume each observation has equal variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "ntrial = 10000   # how many trials will we be doing?\n",
    "nsample = 20   # how many points in each trial\n",
    "sd = c(1:20)   # how does the variance change\n",
    "mu = 2.0\n",
    "\n",
    "get.sample <- function() {\n",
    "  return(rnorm(nsample)*sd + mu)\n",
    "}\n",
    "\n",
    "unweighted.estimate <- function(cur.sample) {\n",
    "  return(mean(cur.sample))\n",
    "}\n",
    "\n",
    "unweighted.estimate <- numeric(ntrial)\n",
    "weighted.estimate <- numeric(ntrial)\n",
    "suboptimal.estimate <- numeric(ntrial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's simulate a number of experiments and compare the three estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "set.seed(0)\n",
    "for (i in 1:ntrial) {\n",
    "  cur.sample = get.sample()\n",
    "  unweighted.estimate[i] = mean(cur.sample)\n",
    "  weighted.estimate[i] = sum(cur.sample/sd^2) / sum(1/sd^2)\n",
    "  suboptimal.estimate[i] = sum(cur.sample/sd) / sum(1/sd)\n",
    "}\n",
    "\n",
    "print(data.frame(mean(unweighted.estimate),\n",
    "                 sd(unweighted.estimate)))\n",
    "print(data.frame(mean(weighted.estimate),\n",
    "                 sd(weighted.estimate)))\n",
    "print(data.frame(mean(suboptimal.estimate),\n",
    "                 sd(suboptimal.estimate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "Y = c(density(unweighted.estimate)$y, density(weighted.estimate)$y, density(suboptimal.estimate)$y)\n",
    "X = c(density(unweighted.estimate)$x, density(weighted.estimate)$x, density(suboptimal.estimate)$x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%R -h 800 -w 800\n",
    "plot(X, Y, type='n', main='Comparison of densities of the estimators')\n",
    "lines(density(weighted.estimate), col='red', lwd=4)\n",
    "lines(density(unweighted.estimate), col='blue', lwd=4)\n",
    "lines(density(suboptimal.estimate), col='purple', lwd=4)\n",
    "legend(6,0.3, c('optimal', 'suboptimal', 'mean'), col=c('red', 'purple', 'blue'), lwd=rep(4,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
